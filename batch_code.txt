# --- Batch Processing 500 Images (USE_COMBO=True) ---
import itertools
import hashlib
import json

# GT not pertinent : 1, 39, 42, 152, 203, 204, 206, 397, 411, 414, 415, 431, 449, 452, 457, 460, 461, 465, 469, 471, 475, 478
# Illustration : 203 ou 206
# Question : 490 ?!?! 
excluded_ids = [1, 39, 42, 133, 152, 203, 204, 206, 397, 411, 414, 415, 431, 449, 452, 457, 460, 461, 465, 469, 471, 475, 478]
excluded_ids = [idx -1 for idx in excluded_ids]

start_idx = 0
end_idx = 500
n_jobs = 8
USE_COMBO = False
COMPUTE_RESULTS = False # Set to True to re-run heavy computations

# Parameters dict for logging
params_log = {
    "sigma": Σ,
    "beta": β,
    "c": c,
    "c_theta": c_θ,
    "R": R,
    "K": K,
    "threshold_mask": threshold_mask,
    "f_threshold": f_threshold,
    "min_centrality": min_centrality,
    "weights": weights,
    "dark_ridges": dark_ridges
}

# Generate descriptive name
w_str = "-".join([f"{k[0]}{v:.2f}" for k,v in weights.items()])
run_name = f"Batch_beta{β}_R{R}_w{w_str}"

base_output_dir = "/content/drive/MyDrive/Datasets/FIND/Results/Avignon_Notebook_Batch"
output_dir = os.path.join(base_output_dir, run_name)
os.makedirs(output_dir, exist_ok=True)

# Save params
with open(os.path.join(output_dir, "params.json"), "w") as f:
    json.dump(params_log, f, indent=4)

print(f"Results directory: {output_dir}")

def process_image_idx_combo(idx):
    if idx in excluded_ids: return None
    try:
        dat = load_modalities_and_gt_by_index(struct, idx)
        base = dat["arrays"].get("intensity", next(iter(dat["arrays"].values())))
        gt = (dat["arrays"].get("label", np.zeros_like(base)) > 0).astype(np.uint8)
        gt = binary_closing(gt, footprint=disk(2))
        gt = binary_opening(gt, footprint=disk(2))
        sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)

        # --- Load CSD & Compute Metrics ---
        csd_path_in = f"/content/drive/MyDrive/Datasets/FIND/Results/CrackSegDiff/20000_1000/test_output_fused/im{idx+1:05d}_output_ens.png"
        sk_csd_thick = np.zeros_like(sk_gt_thick)
        csd_jac, csd_tvs, csd_wass = 0.0, 0.0, 0.0
        if os.path.exists(csd_path_in):
            try:
                csd_img = np.array(Image.open(csd_path_in).convert('L'))
                csd_bin = (csd_img > 127).astype(np.uint8)
                csd_bin = binary_closing(csd_bin, footprint=disk(2))
                csd_bin = binary_opening(csd_bin, footprint=disk(2))
                sk_csd_thick = thicken(skeletonize_lee(csd_bin), pixels=3)
                csd_jac = jaccard_index(sk_csd_thick, sk_gt_thick)
                csd_tvs = tversky_index(sk_csd_thick, sk_gt_thick, alpha=1.0, beta=0.5)
                csd_wass = wasserstein_distance_skeletons(sk_csd_thick, sk_gt_thick)
            except: pass

        # Pre-compute Hessians
        hessian_cache = {}
        valid_keys = [k for k in weights if k in dat["arrays"] and weights[k] > 0]
        for k in valid_keys:
            arr = to_gray(dat["arrays"][k])
            h_norm = compute_hessians_per_scale(arr, Σ)
            hessian_cache[k] = [h_norm]
            if USE_COMBO:
                h_inv = compute_hessians_per_scale(255 - arr, Σ)
                hessian_cache[k].append(h_inv)

        combo_indices = list(itertools.product([0, 1], repeat=len(valid_keys))) if USE_COMBO else [tuple(0 for _ in valid_keys)]
        best_tversky = -1.0
        best_res = None

        for combo in combo_indices:
            current_mods = {}
            for i, mod in enumerate(valid_keys):
                current_mods[mod] = hessian_cache[mod][combo[i]]
            fused_H = fuse_hessians_per_scale(current_mods, weights)
            coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)
            D = distances_from_similarity(S, mode="minus")
            if K == 2: D = triangle_connectivity_graph(coords, D)
            D_cc, idx_nodes = largest_connected_component(D)
            sk_pred = np.zeros_like(base, dtype=np.uint8)
            if D_cc.shape[0] > 0:
                labels = np.zeros(D_cc.shape[0], dtype=int) # SKIP HDBSCAN
                sub_coords = coords[idx_nodes]
                all_edges = []
                for lab in np.unique(labels):
                    if lab < 0: continue
                    cl = np.where(labels == lab)[0]
                    if cl.size < 3: continue
                    mst = mst_on_cluster(D_cc, cl)
                    global_indices = idx_nodes[cl]
                    S_cluster = S[global_indices, :][:, global_indices]
                    nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True, min_centrality=min_centrality)
                    segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)
                    if segs.shape[0] > 0: all_edges.append(segs)
                if all_edges:
                    fault_edges = np.vstack(all_edges)
                    mask = np.zeros_like(base, dtype=np.uint8)
                    for e in fault_edges:
                        r0, c0, r1, c1, _ = e
                        rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))
                        rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)
                        mask[rr, cc] = 1
                    sk_pred = skeletonize_lee(mask)
            sk_pred_thick = thicken(sk_pred, pixels=3)
            tvs = tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5)
            if tvs > best_tversky:
                best_tversky = tvs
                jac = jaccard_index(sk_pred_thick, sk_gt_thick)
                wass = wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)
                # --- SAVE VISUALIZATIONS ---
                skel_dir = os.path.join(output_dir, "skeleton")
                over_dir = os.path.join(output_dir, "overlay")
                os.makedirs(skel_dir, exist_ok=True)
                os.makedirs(over_dir, exist_ok=True)
                iio.imwrite(os.path.join(skel_dir, f"im{idx+1:05d}_skel.png"), (sk_pred_thick * 255).astype(np.uint8))
                # RGB Overlay
                H_ov, W_ov = sk_gt_thick.shape
                ov_img = np.zeros((H_ov, W_ov, 3), dtype=np.uint8)
                ov_img[..., 0] = np.clip(sk_gt_thick * 255 + sk_pred_thick * 255, 0, 255)
                ov_img[..., 1] = np.clip(sk_gt_thick * 255 + sk_csd_thick * 255, 0, 255)
                ov_img[..., 2] = np.clip(sk_gt_thick * 255, 0, 255)
                iio.imwrite(os.path.join(over_dir, f"im{idx+1:05d}_overlay.png"), ov_img)
                best_res = {
                    "Image": f"im{idx+1:05d}",
                    "Jaccard": jac, "Tversky": tvs, "Wasserstein": wass,
                    "CSD_Jaccard": csd_jac, "CSD_Tversky": csd_tvs, "CSD_Wasserstein": csd_wass,
                    "Combo": str(combo)
                }
        return best_res
    except Exception as e: return None

if COMPUTE_RESULTS:
    print(f"Processing batch {start_idx}-{end_idx}...")
    with tqdm_joblib(tqdm(total=end_idx-start_idx)) as progress_bar:
        results = Parallel(n_jobs=n_jobs)(delayed(process_image_idx_combo)(i) for i in range(start_idx, end_idx))
    results = [r for r in results if r is not None]
    df_res = pd.DataFrame(results)
    if not df_res.empty:
        print("\n--- Results ---")
        mean_vals = df_res[["Jaccard", "Tversky", "Wasserstein", "CSD_Jaccard", "CSD_Tversky", "CSD_Wasserstein"]].mean()
        print("--- Mean Results ---")
        print(mean_vals)
        mean_row = mean_vals.to_dict()
        mean_row["Image"] = "MEAN"
        mean_row["Combo"] = "N/A"
        df_final = pd.concat([df_res, pd.DataFrame([mean_row])], ignore_index=True)
        df_final.to_csv(os.path.join(output_dir, "metrics_combo.csv"), index=False)
    else: print("No valid results.")
else:
    print("COMPUTE_RESULTS is False. Skipping batch computation.")
    csv_path = os.path.join(output_dir, "metrics_combo.csv")
    if os.path.exists(csv_path):
        print(f"Loading results from {csv_path}")
        df_final = pd.read_csv(csv_path)
        print("--- Loaded Mean Results ---")
        print(df_final.tail(1))
    else:
        print(f"No existing results found at {csv_path}")
