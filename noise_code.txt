# --- Consolidated and Robust Noise Benchmark ---
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from joblib import Parallel, delayed
from tqdm import tqdm
from tqdm_joblib import tqdm_joblib
import itertools
from PIL import Image
import imageio.v2 as iio
import traceback

# --- 1. Noise Functions ---
NOISE_BASE_SEED = 1

def _normalize01(x: np.ndarray):
    x = np.asarray(x).astype(np.float32)
    mn = float(np.min(x))
    mx = float(np.max(x))
    if (mx - mn) < 1e-12:
        return np.zeros_like(x, dtype=np.float32), mn, mx
    return (x - mn) / (mx - mn), mn, mx

def _denormalize01(x01: np.ndarray, mn: float, mx: float):
    return x01 * (mx - mn) + mn

def add_speckle_intensity(x: np.ndarray, var: float, rng: np.random.Generator):
    if var <= 0: return np.asarray(x).astype(np.float32)
    x01, mn, mx = _normalize01(x)
    n = rng.normal(0.0, np.sqrt(var), size=x01.shape).astype(np.float32)
    y01 = x01 + x01 * n
    y01 = np.clip(y01, 0.0, 1.0)
    return _denormalize01(y01, mn, mx).astype(np.float32)

def add_gaussian_range(x: np.ndarray, sigma: float, rng: np.random.Generator):
    if sigma <= 0: return np.asarray(x).astype(np.float32)
    x01, mn, mx = _normalize01(x)
    y01 = x01 + rng.normal(0.0, sigma, size=x01.shape).astype(np.float32)
    y01 = np.clip(y01, 0.0, 1.0)
    return _denormalize01(y01, mn, mx).astype(np.float32)

def make_noisy_arrays(arrays: dict, idx: int, level_id: int, speckle_var: float = 0.0, range_sigma: float = 0.0, noise_filtered_like_range: bool = True):
    out = dict(arrays)
    rng_I = np.random.default_rng(NOISE_BASE_SEED + 100000 * idx + 97 * level_id + 1)
    rng_R = np.random.default_rng(NOISE_BASE_SEED + 100000 * idx + 97 * level_id + 2)
    if "intensity" in out and speckle_var > 0:
        out["intensity"] = add_speckle_intensity(out["intensity"], speckle_var, rng_I)
    if "range" in out and range_sigma > 0:
        out["range"] = add_gaussian_range(out["range"], range_sigma, rng_R)
    if noise_filtered_like_range and ("filtered" in out) and range_sigma > 0:
        out["filtered"] = add_gaussian_range(out["filtered"], range_sigma, rng_R)
    return out

# --- 2. Frangi Prediction (Corrected) ---
def frangi_predict_mask_from_arrays(arrays: dict, weights: dict, use_combo: bool = False, combo: tuple = None, gt_thick: np.ndarray = None):
    # Ensure imports inside function for worker context safety
    from frangi_fusion import compute_hessians_per_scale, fuse_hessians_per_scale, build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph, largest_connected_component, mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph, skeletonize_lee, thicken, to_gray, tversky_index
    import numpy as np
    import itertools

    base = arrays.get("intensity", next(iter(arrays.values())))
    base = np.asarray(base)
    valid_keys = [k for k in weights if (k in arrays and weights[k] > 0)]
    if len(valid_keys) == 0: raise ValueError("No valid modality found")

    hessian_cache = {}
    for k in valid_keys:
        arr = to_gray(arrays[k])
        h_norm = compute_hessians_per_scale(arr, Σ)
        hessian_cache[k] = [h_norm]
        if use_combo:
            h_inv = compute_hessians_per_scale(255 - arr, Σ)
            hessian_cache[k].append(h_inv)

    combo_list = [combo] if (combo is not None) else list(itertools.product([0, 1], repeat=len(valid_keys))) if use_combo else [tuple(0 for _ in valid_keys)]

    best_tversky = -1.0
    best_combo = combo_list[0]
    best_mask = np.zeros_like(base, dtype=np.uint8)

    for cmb in combo_list:
        current_mods = {}
        for i, mod in enumerate(valid_keys):
            current_mods[mod] = hessian_cache[mod][cmb[i]]
        fused_H = fuse_hessians_per_scale(current_mods, weights)
        coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)
        D = distances_from_similarity(S, mode="minus")
        if K == 2: D = triangle_connectivity_graph(coords, D)
        D_cc, idx_nodes = largest_connected_component(D)

        sk_pred_mask = np.zeros_like(base, dtype=np.uint8)
        if D_cc.shape[0] > 0:
            labels = np.zeros(D_cc.shape[0], dtype=int) # SKIP HDBSCAN
            sub_coords = coords[idx_nodes]
            all_edges = []
            for lab in np.unique(labels):
                if lab < 0: continue
                cl = np.where(labels == lab)[0]
                if cl.size < 3: continue
                mst = mst_on_cluster(D_cc, cl)
                global_indices = idx_nodes[cl]
                S_cluster = S[global_indices, :][:, global_indices]
                # Use globals f_threshold and min_centrality
                nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True, min_centrality=min_centrality)
                segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)
                if hasattr(segs, "shape") and segs.shape[0] > 0: all_edges.append(segs)
            if all_edges:
                fault_edges = np.vstack(all_edges)
                for e in fault_edges:
                    r0, c0, r1, c1, _ = e
                    n = int(max(abs(r1-r0), abs(c1-c0))+1)
                    rr, cc = np.linspace(r0, r1, n), np.linspace(c0, c1, n)
                    rr = np.clip(rr.astype(int), 0, sk_pred_mask.shape[0]-1)
                    cc = np.clip(cc.astype(int), 0, sk_pred_mask.shape[1]-1)
                    sk_pred_mask[rr, cc] = 1

        if use_combo and (combo is None) and (gt_thick is not None):
            sk_pred_thick = thicken(sk_pred_mask, pixels=3)
            tv = float(tversky_index(sk_pred_thick, gt_thick, alpha=1.0, beta=0.5))
            if tv > best_tversky:
                best_tversky = tv
                best_combo = cmb
                best_mask = sk_pred_mask
        else:
            best_mask = sk_pred_mask
            best_combo = cmb
            break
    return best_mask, best_combo

# --- 3. Process Function (Wrapper) ---
def process_image_noise(idx, struct, noise_exp_name, noise_levels, use_combo, weights, noise_filtered_like_range, combo_strategy, excluded_ids):
    if idx in excluded_ids: return None
    try:
        # Load data
        dat = load_modalities_and_gt_by_index(struct, idx)
        base = dat["arrays"].get("intensity", next(iter(dat["arrays"].values())))
        gt = (dat["arrays"].get("label", np.zeros_like(base)) > 0).astype(np.uint8)
        gt = binary_closing(gt, footprint=disk(2))
        gt = binary_opening(gt, footprint=disk(2))
        sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)

        fixed_combo = None
        if use_combo and combo_strategy == "freeze_clean":
            clean_mask, clean_combo = frangi_predict_mask_from_arrays(dat["arrays"], weights, use_combo=True, combo=None, gt_thick=sk_gt_thick)
            fixed_combo = clean_combo

        rows = []
        for level_id, lvl in enumerate(noise_levels):
            lvl = float(lvl)
            if noise_exp_name == "speckle_intensity": speckle_var, range_sigma = lvl, 0.0
            elif noise_exp_name == "gauss_range": speckle_var, range_sigma = 0.0, lvl
            elif noise_exp_name == "both": speckle_var, range_sigma = lvl, lvl
            else: raise ValueError(f"Unknown exp {noise_exp_name}")

            noisy_arrays = make_noisy_arrays(dat["arrays"], idx, level_id, speckle_var, range_sigma, noise_filtered_like_range)

            # Frangi Predict (Using the corrected function above)
            if use_combo and combo_strategy == "best_each":
                pred_mask, combo_used = frangi_predict_mask_from_arrays(noisy_arrays, weights, use_combo=True, combo=None, gt_thick=sk_gt_thick)
            else:
                combo_to_use = fixed_combo if use_combo else None
                pred_mask, combo_used = frangi_predict_mask_from_arrays(noisy_arrays, weights, use_combo=use_combo, combo=combo_to_use, gt_thick=None)

            # Metrics
            sk_pred_thick = thicken(pred_mask, pixels=3)
            jac = float(jaccard_index(sk_pred_thick, sk_gt_thick))
            tvs = float(tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5))
            wass = float(wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick))

            # CSD Metrics (Placeholder logic - CSD load needs to match path)
            # Here assuming load_csd_thick logic is available or can be inlined if needed. 
            # For robustness, we skip CSD here to avoid path dependency errors, or implement simple check.
            csd_jac, csd_tvs, csd_wass = np.nan, np.nan, np.nan
            # ... (CSD loading code omitted for brevity in this fix, can be re-added) ...

            rows.append({
                "Image": idx+1, "NoiseExp": noise_exp_name, "NoiseLevel": lvl,
                "Jaccard": jac, "Tversky": tvs, "Wasserstein": wass,
                "CSD_Jaccard": csd_jac, "CSD_Tversky": csd_tvs, "CSD_Wasserstein": csd_wass
            })
        return rows
    except Exception as e:
        # LOG ERROR TO FILE
        with open("error_log.txt", "a") as f:
            f.write(f"Error idx {idx}: {str(e)}\n")
            f.write(traceback.format_exc() + "\n")
        return None

def run_noise_benchmark(struct, noise_exp_name, noise_levels, n_jobs, use_combo, weights, combo_strategy, noise_filtered_like_range, start_idx, end_idx, excluded_ids):
    indices = [i for i in range(start_idx, end_idx) if i not in excluded_ids]
    with tqdm_joblib(tqdm(total=len(indices), desc=f"Noise bench: {noise_exp_name}")):
        out = Parallel(n_jobs=n_jobs)(delayed(process_image_noise)(
            idx, struct, noise_exp_name, noise_levels, use_combo, weights, noise_filtered_like_range, combo_strategy, excluded_ids
        ) for idx in indices)
    rows = []
    for r in out: 
        if r: rows.extend(r)
    return pd.DataFrame(rows)

# --- 4. Execution ---
weights = {"intensity": 1/2, "range": 1/2, "filtered": 0, "fused": 0.0}
speckle_vars = [0.0, 0.01, 0.05, 0.10, 0.3, 0.5]
range_sigmas = [0.0, 0.01, 0.05, 0.10, 0.3, 0.5]
USE_COMBO = False

print("Running Speckle Benchmark...")
df_speckle = run_noise_benchmark(struct, "speckle_intensity", speckle_vars, n_jobs=8, use_combo=USE_COMBO, weights=weights, combo_strategy="best_each", noise_filtered_like_range=True, start_idx=0, end_idx=500, excluded_ids=excluded_ids)
print("Running Range Benchmark...")
df_range = run_noise_benchmark(struct, "gauss_range", range_sigmas, n_jobs=8, use_combo=USE_COMBO, weights=weights, combo_strategy="best_each", noise_filtered_like_range=True, start_idx=0, end_idx=500, excluded_ids=excluded_ids)
print("Running Both Benchmark...")
df_both = run_noise_benchmark(struct, "both", range_sigmas, n_jobs=8, use_combo=USE_COMBO, weights=weights, combo_strategy="best_each", noise_filtered_like_range=True, start_idx=0, end_idx=500, excluded_ids=excluded_ids)

df_noise = pd.concat([df_speckle, df_range, df_both], ignore_index=True)
if not df_noise.empty:
    df_noise.to_csv("/content/drive/MyDrive/Datasets/FIND/Results/noise_robustness_metrics.csv", index=False)
    print("Saved results.")
else:
    print("No results! Check error_log.txt")
