{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT44ifHxx436"
      },
      "source": [
        "# Universal and Robust Multi-Modal Crack Extraction via Generalized Frangi Graphs and Topological Centrality\n",
        "\n",
        "**Abstract**\n",
        "Automatic crack detection is a pivotal task in structural health monitoring and geoscience. We propose a \"universal\", training-free approach that robustly extracts crack networks across varying data distributions. Our method generalizes the classical Frangi vesselness filter to the multi-modal setting, fusing photometric (intensity) and geometric (range/depth) data at the Hessian level. Instead of pixel-wise classification, we construct a sparse graph driven by a pairwise Frangi similarity metric. A novel topological extraction algorithm—combining HDBSCAN and Weighted Betweenness Centrality on a MST—isolates the precise topological skeleton.\n",
        "\n",
        "This notebook reproduces the experiments presented in the paper (ICPR 2025, Lyon), illustrating the method on:\n",
        "1.  **Real-world geological data**: The *Palais des Papes* in Avignon.\n",
        "2.  **The FIND Benchmark**: A dataset of 500 registered intensity and range images.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dDAB3srx437",
        "outputId": "eeecb37e-eb67-4060-95f0-fa2106fd84e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset'...\n",
            "remote: Enumerating objects: 502, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 502 (delta 56), reused 49 (delta 24), pack-reused 404 (from 1)\u001b[K\n",
            "Receiving objects: 100% (502/502), 40.49 MiB | 36.27 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# 1. Environment Setup\n",
        "!pip -q install numpy scipy scikit-image matplotlib joblib tqdm tqdm-joblib hdbscan networkx gdown tifffile imageio pandas Pillow pot\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\"):\n",
        "    !git clone https://github.com/Ludwig-H/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset.git\n",
        "\n",
        "%cd Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLaYxkZ1x439"
      },
      "outputs": [],
      "source": [
        "# 2. Imports and Global Parameters\n",
        "import os, sys, numpy as np, matplotlib.pyplot as plt\n",
        "import gdown, imageio.v2 as iio, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "from skimage import io, color, img_as_float32\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import binary_closing, binary_opening, disk\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from joblib import Parallel, delayed\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure local src is importable\n",
        "if os.path.exists(\"src\"):\n",
        "    sys.path.append(os.path.abspath(\"src\"))\n",
        "elif os.path.exists(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/src\"):\n",
        "    sys.path.append(os.path.abspath(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/src\"))\n",
        "\n",
        "from frangi_fusion import (\n",
        "    set_seed, load_modalities_and_gt_by_index, to_gray,\n",
        "    compute_hessians_per_scale, fuse_hessians_per_scale,\n",
        "    build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph,\n",
        "    largest_connected_component, hdbscan_from_sparse,\n",
        "    mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph,\n",
        "    skeletonize_lee, thicken, jaccard_index, tversky_index, wasserstein_distance_skeletons,\n",
        "    auto_discover_find_structure\n",
        ")\n",
        "\n",
        "# --- Global Hyper-parameters ---\n",
        "Σ = [1, 3, 5, 7, 9]  # Gaussian scales\n",
        "β = 0.5    # Frangi blob sensitivity\n",
        "c = 0.25   # Contrast sensitivity\n",
        "c_θ = 0.125 # Orientation sensitivity\n",
        "R = 8      # Graph neighbor radius\n",
        "K = 1      # 1 = Standard, 2 = Triangle Connectivity\n",
        "threshold_mask = 0.75\n",
        "dark_ridges = True\n",
        "\n",
        "expZ = 1\n",
        "min_cluster_size = 512\n",
        "max_dist = 1\n",
        "min_samples = 1\n",
        "allow_single_cluster = True\n",
        "\n",
        "f_threshold = 0.25\n",
        "\n",
        "print(\"Libraries loaded and parameters defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K466jt65x43-"
      },
      "source": [
        "## Methodology: Hessian Fusion and Generalized Frangi Graph\n",
        "\n",
        "### 1. Multi-Modal Hessian Fusion\n",
        "For each modality $m$ (e.g., Intensity, Depth) and scale $\\sigma \\in \\Sigma$, we compute the normalized Hessian matrix $\\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x})$ at pixel $\\mathbf{x}$. To handle disparate dynamic ranges, we normalize by the maximum spectral norm:\n",
        "$$ \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) = \\frac{\\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{x})}{\\max_{\\mathbf{y} \\in \\Omega} \\| \\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{y}) \\|} $$\n",
        "The **Fused Hessian** is a weighted linear combination:\n",
        "$$ \\mathcal{H}_{\\sigma}^{\\text{fused}}(\\mathbf{x}) = \\sum_{m} w_m \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) $$\n",
        "We analyze eigenvalues $\\lambda_1, \\lambda_2$ (with $|\\lambda_1| \\le |\\lambda_2|$) and eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2$. For cracks (dark ridges), we expect $\\lambda_2 > 0$.\n",
        "\n",
        "### 2. Generalized Frangi Similarity\n",
        "We construct a graph where edges connect pixels $i, j$ within radius $R=5$. The pairwise similarity $S_{ij} ∈ [0,1]$ enforces local tubular geometry:\n",
        "\n",
        "1.  **Elongation ($S_{\\text{shape}}$):** Using ratio $\\mathcal{R}_B = |\\lambda_1| / |\\lambda_2|$:\n",
        "$$ S_{\\text{shape}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\mathcal{R}_B(\\mathbf{x}_i) + \\mathcal{R}_B(\\mathbf{x}_j)}{\\beta}\\right)^2\\right) $$\n",
        "2.  **Contrast ($S_{\\text{int}}$):** Using energy $\\mathcal{S} = \\|\\mathcal{H}\\|$:\n",
        "$$ S_{\\text{int}} = 1 - \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{\\mathcal{S}(\\mathbf{x}_i) \\cdot \\mathcal{S}(\\mathbf{x}_j)}}{c}\\right)^2\\right) $$\n",
        "3.  **Alignment ($S_{\\text{align}}$):** Penalizing deviation from $\\mathbf{v}_1$:\n",
        "$$ S_{\\text{align}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sin(\\delta_\\theta)}{c_\\theta}\\right)^2\\right) $$\n",
        "\n",
        "The final distance metric is $d_{ij} = (1 - S_{ij}) \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$.\n",
        "\n",
        "### 3. Topological Extraction\n",
        "The graph is processed sequentially: **HDBSCAN Clustering**, **MST**, and **Weighted Betweenness Centrality ($C_B$)** to extract the skeleton:\n",
        "$$ C_B(v) = \\sum_{s ≠ v ≠ t} \\frac{\\eta_{st}(v)}{\\eta_{st}} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Y3FI22x43-"
      },
      "source": [
        "## Part 1: Case Study - Palais des Papes (Avignon)\n",
        "We demonstrate the power of fusion on a challenging real-world case: the retaining rock of the Palais des Papes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVPb7GQ1x43_"
      },
      "outputs": [],
      "source": [
        "# Download Avignon Data\n",
        "avignon_dir = \"data_avignon\"\n",
        "os.makedirs(avignon_dir, exist_ok=True)\n",
        "ortho_id = \"1OXK3XNdrirwvnwI5yZI2AKlh3qSiSeDd\"\n",
        "mne_id = \"1iB6RllC9augWlAshqE2e08vnglHHM_PQ\"\n",
        "ortho_path = os.path.join(avignon_dir, \"Ortho_new_extrait.tif\")\n",
        "mne_path = os.path.join(avignon_dir, \"MNE_new_extrait.mat\")\n",
        "\n",
        "if not os.path.exists(ortho_path):\n",
        "    gdown.download(id=ortho_id, output=ortho_path, quiet=False)\n",
        "if not os.path.exists(mne_path):\n",
        "    gdown.download(id=mne_id, output=mne_path, quiet=False)\n",
        "print(\"Avignon data downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxoOVHRdx44A"
      },
      "outputs": [],
      "source": [
        "# Helper function to load .mat files\n",
        "def load_mat_2d(path, key=None, dtype=np.float32, squeeze=True):\n",
        "    path = Path(path)\n",
        "    try:\n",
        "        from scipy.io import loadmat\n",
        "        mdict = loadmat(path)\n",
        "        user_keys = [k for k in mdict.keys() if not k.startswith('__')]\n",
        "        if key is None: key = user_keys[0]\n",
        "        arr = mdict[key]\n",
        "    except NotImplementedError:\n",
        "        import h5py\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            available = list(f.keys())\n",
        "            if key is None: key = available[0]\n",
        "            arr = np.array(f[key])\n",
        "    if squeeze: arr = np.squeeze(arr)\n",
        "    return np.asarray(arr, dtype=dtype)\n",
        "\n",
        "# Load and Preprocess\n",
        "IMG_PATH = ortho_path\n",
        "img = io.imread(IMG_PATH)\n",
        "if img.ndim == 2: img_gray = img_as_float32(img)\n",
        "else: img_gray = img_as_float32(color.rgb2gray(img))\n",
        "\n",
        "Scale_Z = 100.0\n",
        "F = 5 # Downsampling factor for speed\n",
        "depth_raw = load_mat_2d(mne_path, key=\"mne\")\n",
        "depth_map = Scale_Z / F * depth_raw\n",
        "\n",
        "# Resize\n",
        "if img_gray.shape != depth_map.shape:\n",
        "    depth_map = resize(depth_map, img_gray.shape, order=1, preserve_range=True)\n",
        "new_shape = (img_gray.shape[0] // F, img_gray.shape[1] // F)\n",
        "img_gray_small = resize(img_gray, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "depth_map_small = resize(depth_map, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "\n",
        "print(f\"Processed shapes: Image {img_gray_small.shape}, Depth {depth_map_small.shape}\")\n",
        "\n",
        "# Visualize Inputs\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img_gray_small, cmap='gray'); ax[0].set_title(\"Intensity\")\n",
        "ax[1].imshow(img_gray_small, cmap='gray')\n",
        "im = ax[1].imshow(depth_map_small, cmap='seismic', alpha=0.6)\n",
        "ax[1].set_title(\"Intensity + Depth\")\n",
        "plt.colorbar(im, ax=ax[1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwAtqP4cx44A"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(img_input, modality_name, custom_hessian=None):\n",
        "    # 1. Hessian\n",
        "    if custom_hessian is None:\n",
        "        hessians = compute_hessians_per_scale(img_input, Σ)\n",
        "        mods = {modality_name: hessians}\n",
        "        weights = {modality_name: 1.0}\n",
        "        fused_H = fuse_hessians_per_scale(mods, weights)\n",
        "    else:\n",
        "        fused_H = custom_hessian\n",
        "\n",
        "    # 2. Graph\n",
        "    coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "    D = distances_from_similarity(S, mode=\"minus\")\n",
        "    if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "\n",
        "    # 3. Extraction\n",
        "    D_cc, idx_nodes = largest_connected_component(D)\n",
        "    mask = np.zeros_like(img_input)\n",
        "    if D_cc.shape[0] > 0:\n",
        "        Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "        labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "        sub_coords = coords[idx_nodes]\n",
        "        all_edges = []\n",
        "        for lab in np.unique(labels):\n",
        "            if lab < 0: continue\n",
        "            cl = np.where(labels == lab)[0]\n",
        "            if cl.size < 3: continue\n",
        "            mst = mst_on_cluster(D_cc, cl)\n",
        "            global_indices = idx_nodes[cl]\n",
        "            S_cluster = S[global_indices, :][:, global_indices]\n",
        "            nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "            segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "            if segs.shape[0] > 0: all_edges.append(segs)\n",
        "        if all_edges:\n",
        "            fault_edges = np.vstack(all_edges)\n",
        "            for e in fault_edges:\n",
        "                r0, c0, r1, c1, _ = e\n",
        "                rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                mask[rr, cc] = 1.0\n",
        "    return fused_H, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvFRu9e4x44B"
      },
      "outputs": [],
      "source": [
        "print(\"Processing Intensity...\")\n",
        "H_int, mask_int = run_pipeline(img_gray_small, \"intensity\")\n",
        "\n",
        "print(\"Processing Depth...\")\n",
        "H_depth, mask_depth = run_pipeline(depth_map_small, \"depth\")\n",
        "\n",
        "print(\"Processing Fusion...\")\n",
        "mods_fusion = {\n",
        "    \"intensity\": compute_hessians_per_scale(img_gray_small, Σ),\n",
        "    \"depth\": compute_hessians_per_scale(depth_map_small, Σ)\n",
        "}\n",
        "weights_fusion = {\"intensity\": 0.66, \"depth\": 0.33}\n",
        "fused_H_final = fuse_hessians_per_scale(mods_fusion, weights_fusion)\n",
        "_, mask_fusion = run_pipeline(img_gray_small, \"fused\", custom_hessian=fused_H_final)\n",
        "\n",
        "# Visualize\n",
        "def get_best_lambda2(hess_list):\n",
        "    e2n_stack = np.stack([Hd['e2n'] for Hd in hess_list], axis=0)\n",
        "    abs_e2n_stack = np.abs(e2n_stack)\n",
        "    best_idx = abs_e2n_stack.argmax(axis=0)\n",
        "    return np.take_along_axis(e2n_stack, best_idx[None,...], axis=0)[0]\n",
        "\n",
        "l2_int = get_best_lambda2(H_int)\n",
        "l2_depth = get_best_lambda2(H_depth)\n",
        "l2_fusion = get_best_lambda2(fused_H_final)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes[0,0].imshow(np.clip(l2_int,0,1), cmap='magma'); axes[0,0].set_title(\"Intensity |λ2|\")\n",
        "axes[0,1].imshow(np.clip(l2_depth,0,1), cmap='magma'); axes[0,1].set_title(\"Depth |λ2|\")\n",
        "axes[0,2].imshow(np.clip(l2_fusion,0,1), cmap='magma'); axes[0,2].set_title(\"Fusion |λ2|\")\n",
        "axes[1,0].imshow(mask_int, cmap='gray'); axes[1,0].set_title(\"Result Int\")\n",
        "axes[1,1].imshow(mask_depth, cmap='gray'); axes[1,1].set_title(\"Result Depth\")\n",
        "axes[1,2].imshow(mask_fusion, cmap='Reds'); axes[1,2].set_title(\"Result Fusion\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzE76OM7x44C"
      },
      "source": [
        "## Part 2: FIND Dataset Benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_ENHluWMgx0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GRKaM3GMgx0"
      },
      "outputs": [],
      "source": [
        "# Download FIND Dataset if not present\n",
        "import zipfile\n",
        "url = \"https://drive.google.com/uc?id=1qnLMCeon7LJjT9H0ENiNF5sFs-F7-NvK\"\n",
        "zip_path = \"data.zip\"\n",
        "extract_dir = \"data_find\"\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    if not os.path.exists(zip_path):\n",
        "        gdown.download(url, zip_path, quiet=False)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Unzipped FIND dataset.\")\n",
        "else:\n",
        "    print(\"FIND dataset already present.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHONQnmMgx0"
      },
      "outputs": [],
      "source": [
        "# --- Single Example Analysis (Seed=1) ---\n",
        "import hdbscan\n",
        "\n",
        "seed = 1\n",
        "\n",
        "print(f\"Analyzing image index {seed}...\")\n",
        "struct = auto_discover_find_structure(\"data_find\")\n",
        "dat = load_modalities_and_gt_by_index(struct, seed)\n",
        "base = dat[\"arrays\"].get(\"intensity\", next(iter(dat[\"arrays\"].values())))\n",
        "gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "\n",
        "# 1. Compute Hessians & Fusion\n",
        "mods_hess = {}\n",
        "weights = {\"intensity\": 0.4, \"range\": 0.1, \"filtered\": 0.5, \"fused\": 0.0}\n",
        "valid_keys = [k for k in weights if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "for k in valid_keys: mods_hess[k] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][k]), Σ)\n",
        "fused_H = fuse_hessians_per_scale(mods_hess, weights)\n",
        "\n",
        "# 2. Graph Construction\n",
        "coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "D = distances_from_similarity(S, mode=\"minus\")\n",
        "if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "print(f\"Largest CC: {D_cc.shape[0]} nodes.\")\n",
        "\n",
        "# 3. HDBSCAN Clustering\n",
        "sub_coords = coords[idx_nodes]\n",
        "if D_cc.shape[0] > 0:\n",
        "    Dist = D_cc.copy()\n",
        "    Dist.data = Dist.data ** expZ\n",
        "    Dist = Dist.tocsr()\n",
        "    Dist.setdiag(0.0)\n",
        "\n",
        "    # Explicit HDBSCAN call as requested\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=\"precomputed\",\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        max_dist=max_dist,\n",
        "        allow_single_cluster=allow_single_cluster,\n",
        "    )\n",
        "    labels = clusterer.fit_predict(Dist)\n",
        "\n",
        "    # Detailed output\n",
        "    print(\"Clusters:\", np.unique(labels), \". 'Noise':\", np.sum(labels == -1))\n",
        "else:\n",
        "    labels = np.array([])\n",
        "\n",
        "# 4. Skeletonization (MST + Betweenness)\n",
        "all_edges = []\n",
        "sk_pred_mask = np.zeros_like(base, dtype=np.uint8)\n",
        "centrality_vis = np.zeros_like(base, dtype=np.float32) # VISU\n",
        "\n",
        "if labels.size > 0:\n",
        "    for lab in np.unique(labels):\n",
        "        if lab < 0: continue\n",
        "        cl = np.where(labels == lab)[0]\n",
        "        if cl.size < 3: continue\n",
        "        mst = mst_on_cluster(D_cc, cl)\n",
        "        global_indices = idx_nodes[cl]\n",
        "        S_cluster = S[global_indices, :][:, global_indices]\n",
        "\n",
        "        # --- VISU CENTRALITY START ---\n",
        "        # Re-compute centrality map for visualization (same logic as in mst_kcenters)\n",
        "        from scipy.sparse.csgraph import breadth_first_order\n",
        "        N_cl = mst.shape[0]\n",
        "        order, predecessors = breadth_first_order(mst, i_start=0, directed=False, return_predecessors=True)\n",
        "        node_weights = np.ones(N_cl, dtype=np.float64)\n",
        "        node_weights[0] = 0.0\n",
        "        for i in range(N_cl):\n",
        "            if i==0: continue\n",
        "            p = predecessors[i]\n",
        "            if p>=0 and p<N_cl: node_weights[i] = S_cluster[p, i]\n",
        "            else: node_weights[i] = 0.0\n",
        "        subtree_mass = node_weights.copy()\n",
        "        for i in order[::-1]:\n",
        "            if i != 0:\n",
        "                p = predecessors[i]\n",
        "                if p>=0 and p<N_cl: subtree_mass[p] += subtree_mass[i]\n",
        "        total_mass = subtree_mass[0]\n",
        "        cent = subtree_mass * (total_mass - subtree_mass)\n",
        "        # Normalize for display\n",
        "        if cent.max() > 0: cent /= cent.max()\n",
        "\n",
        "        # Map back to image\n",
        "        rows, cols = sub_coords[cl][:, 0], sub_coords[cl][:, 1]\n",
        "        centrality_vis[rows, cols] = cent\n",
        "        # --- VISU CENTRALITY END ---\n",
        "\n",
        "        nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "        segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "        if segs.shape[0] > 0: all_edges.append(segs)\n",
        "\n",
        "    if all_edges:\n",
        "        fault_edges = np.vstack(all_edges)\n",
        "        for e in fault_edges:\n",
        "            r0, c0, r1, c1, _ = e\n",
        "            rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "            rr, cc = np.clip(rr.astype(int), 0, sk_pred_mask.shape[0]-1), np.clip(cc.astype(int), 0, sk_pred_mask.shape[1]-1)\n",
        "            sk_pred_mask[rr, cc] = 1\n",
        "\n",
        "# 5. Visualizations\n",
        "l2_stack = np.stack([Hd['e2n'] for Hd in fused_H], axis=0)\n",
        "l2_fused_vis = np.max(np.abs(l2_stack), axis=0)\n",
        "sim_vis = np.zeros_like(base, dtype=np.float32)\n",
        "# if S.shape[0] > 0:\n",
        "degs = np.array(S.max(axis=1).toarray()).flatten()\n",
        "sim_vis[coords[:,0], coords[:,1]] = degs\n",
        "\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Row 1\n",
        "plt.subplot(3, 4, 1); plt.title(\"Intensity\"); plt.imshow(dat[\"arrays\"][\"intensity\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "plt.subplot(3, 4, 2); plt.title(\"Range\"); plt.imshow(dat[\"arrays\"][\"range\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "if 'filtered' in dat['arrays']:\n",
        "    plt.subplot(3, 4, 3); plt.title(\"Filtered Range\"); plt.imshow(dat[\"arrays\"][\"filtered\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "else:\n",
        "    plt.subplot(3, 4, 3); plt.axis(\"off\")\n",
        "plt.subplot(3, 4, 4); plt.title(\"Fused |λ2|\"); plt.imshow(l2_fused_vis, cmap=\"magma\"); plt.axis(\"off\")\n",
        "\n",
        "# Row 2\n",
        "plt.subplot(3, 4, 5); plt.title(\"Frangi Similarity (Max)\"); plt.imshow(sim_vis, cmap=\"inferno\"); plt.axis(\"off\")\n",
        "ax_cl = plt.subplot(3, 4, 6); ax_cl.set_title(\"HDBSCAN Clusters\")\n",
        "ax_cl.imshow(base, cmap='gray', alpha=0.5)\n",
        "if labels.size > 0:\n",
        "    # Separate noise and clusters for visualization\n",
        "    noise_mask = labels == -1\n",
        "    cluster_mask = labels >= 0\n",
        "    if np.any(noise_mask):\n",
        "        ax_cl.scatter(sub_coords[noise_mask, 1], sub_coords[noise_mask, 0], c='k', s=0.5, alpha=0.3, label='Noise')\n",
        "    if np.any(cluster_mask):\n",
        "        ax_cl.scatter(sub_coords[cluster_mask, 1], sub_coords[cluster_mask, 0], c=labels[cluster_mask], cmap='tab20', s=1)\n",
        "ax_cl.axis('off')\n",
        "\n",
        "# Centrality Visu (Log scale + Colorbar)\n",
        "ax_cent = plt.subplot(3, 4, 7); ax_cent.set_title(\"Betweenness Centrality (Log)\")\n",
        "im_cent = ax_cent.imshow(np.log1p(centrality_vis), cmap=\"plasma\")\n",
        "plt.colorbar(im_cent, ax=ax_cent, fraction=0.046, pad=0.04)\n",
        "ax_cent.axis('off')\n",
        "\n",
        "# Ours Skeleton\n",
        "ax_sk = plt.subplot(3, 4, 8); ax_sk.set_title(\"Ours: Skeleton Overlay\")\n",
        "ax_sk.imshow(base, cmap='gray')\n",
        "ax_sk.imshow(sk_pred_mask, cmap='Reds', alpha=0.7, vmin=0, vmax=1)\n",
        "ax_sk.axis('off')\n",
        "\n",
        "# Ground Truth\n",
        "plt.subplot(3, 4, 9); plt.title(\"Ground Truth\"); plt.imshow(gt, cmap=\"gray\"); plt.axis(\"off\")\n",
        "\n",
        "# Row 3\n",
        "csd_path = f\"/content/drive/MyDrive/Datasets/FIND/Results/CrackSegDiff/20000_1000/test_output_fused/im{seed+1:05d}_output_ens.png\"\n",
        "if os.path.exists(csd_path):\n",
        "    try:\n",
        "        csd_img = np.array(Image.open(csd_path).convert('L'))\n",
        "        csd_bin = (csd_img > 127).astype(np.uint8)\n",
        "        # CrackSegDiff - Position 10\n",
        "        plt.subplot(3, 4, 10); plt.title(\"CrackSegDiff\"); plt.imshow(csd_bin, cmap=\"Greens\"); plt.axis(\"off\")\n",
        "\n",
        "        # Overlay - Position 11\n",
        "        ax_ov = plt.subplot(3, 4, 11); ax_ov.set_title(\"Overlay: GT(W)+Ours(R)+CSD(G)\")\n",
        "        ax_ov.imshow(np.zeros_like(base), cmap='gray')\n",
        "        ax_ov.imshow(gt, cmap='Greys', alpha=0.5)\n",
        "        ax_ov.imshow(sk_pred_mask, cmap='Reds', alpha=0.5)\n",
        "        ax_ov.imshow(csd_bin, cmap='Greens', alpha=0.3)\n",
        "        ax_ov.axis('off')\n",
        "    except Exception as e: print(\"Error loading CSD:\", e)\n",
        "else:\n",
        "    print(\"CrackSegDiff result not found at:\", csd_path)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Metrics for this image\n",
        "sk_pred_thick = thicken(sk_pred_mask, pixels=3)\n",
        "sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)\n",
        "print(\"--- Metrics (Seed 1) ---\")\n",
        "print(\"Jaccard:\", jaccard_index(sk_pred_thick, sk_gt_thick))\n",
        "print(\"Tversky:\", tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5))\n",
        "print(\"Wasserstein:\", wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duPkt3E1Mgx1"
      },
      "outputs": [],
      "source": [
        "# --- Batch Processing 500 Images (USE_COMBO=True) ---\n",
        "import itertools\n",
        "\n",
        "excluded_ids = []\n",
        "start_idx = 0\n",
        "end_idx = 500\n",
        "n_jobs = 8\n",
        "USE_COMBO = True\n",
        "allow_single_cluster = True\n",
        "output_dir = \"/content/drive/MyDrive/Datasets/FIND/Results/Avignon_Notebook_Batch\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Results will be saved to: {output_dir}\")\n",
        "\n",
        "def process_image_idx_combo(idx):\n",
        "    if idx in excluded_ids: return None\n",
        "    try:\n",
        "        dat = load_modalities_and_gt_by_index(struct, idx)\n",
        "        base = dat[\"arrays\"].get(\"intensity\", next(iter(dat[\"arrays\"].values())))\n",
        "        gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "        gt = binary_closing(gt, footprint=disk(2))\n",
        "        gt = binary_opening(gt, footprint=disk(2))\n",
        "        sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)\n",
        "\n",
        "        # Pre-compute Hessians (Normal & Inverted)\n",
        "        hessian_cache = {}\n",
        "        valid_keys = [k for k in weights if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "        for k in valid_keys:\n",
        "            arr = to_gray(dat[\"arrays\"][k])\n",
        "            h_norm = compute_hessians_per_scale(arr, Σ)\n",
        "            hessian_cache[k] = [h_norm]\n",
        "            if USE_COMBO:\n",
        "                h_inv = compute_hessians_per_scale(255 - arr, Σ)\n",
        "                hessian_cache[k].append(h_inv)\n",
        "\n",
        "        # Combinations\n",
        "        combo_indices = list(itertools.product([0, 1], repeat=len(valid_keys))) if USE_COMBO else [tuple(0 for _ in valid_keys)]\n",
        "\n",
        "        best_tversky = -1.0\n",
        "        best_res = None\n",
        "\n",
        "        for combo in combo_indices:\n",
        "            current_mods = {}\n",
        "            for i, mod in enumerate(valid_keys):\n",
        "                current_mods[mod] = hessian_cache[mod][combo[i]]\n",
        "\n",
        "            fused_H = fuse_hessians_per_scale(current_mods, weights)\n",
        "            coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "            D = distances_from_similarity(S, mode=\"minus\")\n",
        "            if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "            D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "            sk_pred = np.zeros_like(base, dtype=np.uint8)\n",
        "            if D_cc.shape[0] > 0:\n",
        "                expZ = 1\n",
        "                Dist = D_cc.copy()\n",
        "                Dist.data = Dist.data ** expZ\n",
        "                Dist = Dist.tocsr()\n",
        "                Dist.setdiag(0.0)\n",
        "\n",
        "                # Explicit HDBSCAN call\n",
        "                clusterer = hdbscan.HDBSCAN(\n",
        "                    metric=\"precomputed\",\n",
        "                    min_cluster_size=min_cluster_size,\n",
        "                    min_samples=min_samples,\n",
        "                    max_dist=max_dist,\n",
        "                    allow_single_cluster=allow_single_cluster,\n",
        "                )\n",
        "                labels = clusterer.fit_predict(Dist)\n",
        "\n",
        "                sub_coords = coords[idx_nodes]\n",
        "                all_edges = []\n",
        "                for lab in np.unique(labels):\n",
        "                    if lab < 0: continue\n",
        "                    cl = np.where(labels == lab)[0]\n",
        "                    if cl.size < 3: continue\n",
        "                    mst = mst_on_cluster(D_cc, cl)\n",
        "                    global_indices = idx_nodes[cl]\n",
        "                    S_cluster = S[global_indices, :][:, global_indices]\n",
        "                    nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "                    segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "                    if segs.shape[0] > 0: all_edges.append(segs)\n",
        "                if all_edges:\n",
        "                    fault_edges = np.vstack(all_edges)\n",
        "                    mask = np.zeros_like(base, dtype=np.uint8)\n",
        "                    for e in fault_edges:\n",
        "                        r0, c0, r1, c1, _ = e\n",
        "                        rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                        rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                        mask[rr, cc] = 1\n",
        "                    sk_pred = skeletonize_lee(mask)\n",
        "\n",
        "            sk_pred_thick = thicken(sk_pred, pixels=3)\n",
        "            tvs = tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5)\n",
        "\n",
        "            if tvs > best_tversky:\n",
        "                best_tversky = tvs\n",
        "                jac = jaccard_index(sk_pred_thick, sk_gt_thick)\n",
        "                wass = wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)\n",
        "                best_res = {\n",
        "                    \"Image\": f\"im{idx+1:05d}\",\n",
        "                    \"Jaccard\": jac,\n",
        "                    \"Tversky\": tvs,\n",
        "                    \"Wasserstein\": wass,\n",
        "                    \"Combo\": str(combo)\n",
        "                }\n",
        "        return best_res\n",
        "    except Exception as e: return None\n",
        "\n",
        "print(f\"Processing batch {start_idx}-{end_idx}...\")\n",
        "with tqdm_joblib(tqdm(total=end_idx-start_idx)) as progress_bar:\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_image_idx_combo)(i) for i in range(start_idx, end_idx))\n",
        "\n",
        "results = [r for r in results if r is not None]\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(\"\\n--- Results ---\")\n",
        "    print(df_res[[\"Jaccard\", \"Tversky\", \"Wasserstein\"]].mean())\n",
        "    df_res.to_csv(os.path.join(output_dir, \"metrics_combo.csv\"), index=False)\n",
        "else: print(\"No valid results.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}