{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT44ifHxx436"
      },
      "source": [
        "# Universal and Robust Multi-Modal Crack Extraction via Generalized Frangi Graphs and Topological Centrality\n",
        "\n",
        "**Abstract**\n",
        "Automatic crack detection is a pivotal task in structural health monitoring and geoscience. We propose a \"universal\", training-free approach that robustly extracts crack networks across varying data distributions. Our method generalizes the classical Frangi vesselness filter to the multi-modal setting, fusing photometric (intensity) and geometric (range/depth) data at the Hessian level. Instead of pixel-wise classification, we construct a sparse graph driven by a pairwise Frangi similarity metric. A novel topological extraction algorithm—combining HDBSCAN and Weighted Betweenness Centrality on a MST—isolates the precise topological skeleton.\n",
        "\n",
        "This notebook reproduces the experiments presented in the paper (ICPR 2025, Lyon), illustrating the method on:\n",
        "1.  **Real-world geological data**: The *Palais des Papes* in Avignon.\n",
        "2.  **The FIND Benchmark**: A dataset of 500 registered intensity and range images.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dDAB3srx437",
        "outputId": "a0f310f0-b0d9-4914-b8c0-127c451af8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset'...\n",
            "remote: Enumerating objects: 474, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 474 (delta 38), reused 45 (delta 21), pack-reused 404 (from 1)\u001b[K\n",
            "Receiving objects: 100% (474/474), 37.94 MiB | 37.07 MiB/s, done.\n",
            "Resolving deltas: 100% (247/247), done.\n",
            "/content/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
            "Obtaining file:///content/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: tqdm-joblib in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (0.0.5)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (0.8.40)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (3.6.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (5.2.0)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (2025.10.16)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (2.37.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pot in /usr/local/lib/python3.12/dist-packages (from frangi-fusion==0.1.0) (0.9.6.post1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->frangi-fusion==0.1.0) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->frangi-fusion==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown->frangi-fusion==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.12/dist-packages (from hdbscan->frangi-fusion==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->frangi-fusion==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->frangi-fusion==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->frangi-fusion==0.1.0) (2025.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->frangi-fusion==0.1.0) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->frangi-fusion==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->hdbscan->frangi-fusion==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->frangi-fusion==0.1.0) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->frangi-fusion==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->frangi-fusion==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->frangi-fusion==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->frangi-fusion==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->frangi-fusion==0.1.0) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->frangi-fusion==0.1.0) (1.7.1)\n",
            "Building wheels for collected packages: frangi-fusion\n",
            "  Building editable for frangi-fusion (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for frangi-fusion: filename=frangi_fusion-0.1.0-0.editable-py3-none-any.whl size=2789 sha256=8821eb2b38c0920349eb33444ac868d04b2659b31d68837dea163ba183d6d086\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7t0mkkbw/wheels/cf/4d/0b/07f466e36f5ccb3bee5b5dd1c8bb8f8557c5e10cfde7e5996b\n",
            "Successfully built frangi-fusion\n",
            "Installing collected packages: frangi-fusion\n",
            "Successfully installed frangi-fusion-0.1.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Environment Setup\n",
        "!pip -q install numpy scipy scikit-image matplotlib joblib tqdm tqdm-joblib hdbscan networkx gdown tifffile imageio pandas Pillow pot\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\"):\n",
        "    !git clone https://github.com/Ludwig-H/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset.git\n",
        "\n",
        "%cd Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TLaYxkZ1x439",
        "outputId": "2d572173-c946-4f4a-db6f-a447035eeea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded and parameters defined.\n"
          ]
        }
      ],
      "source": [
        "# 2. Imports and Global Parameters\n",
        "import os, sys, numpy as np, matplotlib.pyplot as plt\n",
        "import gdown, imageio.v2 as iio, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "from skimage import io, color, img_as_float32\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import binary_closing, binary_opening, disk\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from joblib import Parallel, delayed\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure local src is importable\n",
        "if os.path.exists(\"src\"):\n",
        "    sys.path.append(os.path.abspath(\"src\"))\n",
        "elif os.path.exists(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/src\"):\n",
        "    sys.path.append(os.path.abspath(\"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/src\"))\n",
        "\n",
        "from frangi_fusion import (\n",
        "    set_seed, load_modalities_and_gt_by_index, to_gray,\n",
        "    compute_hessians_per_scale, fuse_hessians_per_scale,\n",
        "    build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph,\n",
        "    largest_connected_component, hdbscan_from_sparse,\n",
        "    mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph,\n",
        "    skeletonize_lee, thicken, jaccard_index, tversky_index, wasserstein_distance_skeletons,\n",
        "    auto_discover_find_structure\n",
        ")\n",
        "\n",
        "# --- Global Hyper-parameters ---\n",
        "Σ = [1, 3, 5, 7, 9]  # Gaussian scales\n",
        "β = 0.5    # Frangi blob sensitivity\n",
        "c = 0.25   # Contrast sensitivity\n",
        "c_θ = 0.125 # Orientation sensitivity\n",
        "R = 10      # Graph neighbor radius\n",
        "K = 1      # 1 = Standard, 2 = Triangle Connectivity\n",
        "threshold_mask = 0.75\n",
        "dark_ridges = True\n",
        "\n",
        "expZ = 1\n",
        "min_cluster_size = 512\n",
        "max_dist = 1\n",
        "min_samples = 1\n",
        "allow_single_cluster = True\n",
        "\n",
        "f_threshold = 0.75\n",
        "\n",
        "print(\"Libraries loaded and parameters defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K466jt65x43-"
      },
      "source": [
        "## Methodology: Hessian Fusion and Generalized Frangi Graph\n",
        "\n",
        "### 1. Multi-Modal Hessian Fusion\n",
        "For each modality $m$ (e.g., Intensity, Depth) and scale $\\sigma \\in \\Sigma$, we compute the normalized Hessian matrix $\\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x})$ at pixel $\\mathbf{x}$. To handle disparate dynamic ranges, we normalize by the maximum spectral norm:\n",
        "$$ \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) = \\frac{\\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{x})}{\\max_{\\mathbf{y} \\in \\Omega} \\| \\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{y}) \\|} $$\n",
        "The **Fused Hessian** is a weighted linear combination:\n",
        "$$ \\mathcal{H}_{\\sigma}^{\\text{fused}}(\\mathbf{x}) = \\sum_{m} w_m \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) $$\n",
        "We analyze eigenvalues $\\lambda_1, \\lambda_2$ (with $|\\lambda_1| \\le |\\lambda_2|$) and eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2$. For cracks (dark ridges), we expect $\\lambda_2 > 0$.\n",
        "\n",
        "### 2. Generalized Frangi Similarity\n",
        "We construct a graph where edges connect pixels $i, j$ within radius $R=5$. The pairwise similarity $S_{ij} ∈ [0,1]$ enforces local tubular geometry:\n",
        "\n",
        "1.  **Elongation ($S_{\\text{shape}}$):** Using ratio $\\mathcal{R}_B = |\\lambda_1| / |\\lambda_2|$:\n",
        "$$ S_{\\text{shape}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\mathcal{R}_B(\\mathbf{x}_i) + \\mathcal{R}_B(\\mathbf{x}_j)}{\\beta}\\right)^2\\right) $$\n",
        "2.  **Contrast ($S_{\\text{int}}$):** Using energy $\\mathcal{S} = \\|\\mathcal{H}\\|$:\n",
        "$$ S_{\\text{int}} = 1 - \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{\\mathcal{S}(\\mathbf{x}_i) \\cdot \\mathcal{S}(\\mathbf{x}_j)}}{c}\\right)^2\\right) $$\n",
        "3.  **Alignment ($S_{\\text{align}}$):** Penalizing deviation from $\\mathbf{v}_1$:\n",
        "$$ S_{\\text{align}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sin(\\delta_\\theta)}{c_\\theta}\\right)^2\\right) $$\n",
        "\n",
        "The final distance metric is $d_{ij} = (1 - S_{ij}) \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$.\n",
        "\n",
        "### 3. Topological Extraction\n",
        "The graph is processed sequentially: **HDBSCAN Clustering**, **MST**, and **Weighted Betweenness Centrality ($C_B$)** to extract the skeleton:\n",
        "$$ C_B(v) = \\sum_{s ≠ v ≠ t} \\frac{\\eta_{st}(v)}{\\eta_{st}} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Y3FI22x43-"
      },
      "source": [
        "## Part 1: Case Study - Palais des Papes (Avignon)\n",
        "We demonstrate the power of fusion on a challenging real-world case: the retaining rock of the Palais des Papes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVPb7GQ1x43_",
        "outputId": "107de360-528e-44e9-c7fa-729847edc804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OXK3XNdrirwvnwI5yZI2AKlh3qSiSeDd\n",
            "To: /content/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/data_avignon/Ortho_new_extrait.tif\n",
            "100%|██████████| 1.98M/1.98M [00:00<00:00, 24.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1iB6RllC9augWlAshqE2e08vnglHHM_PQ\n",
            "From (redirected): https://drive.google.com/uc?id=1iB6RllC9augWlAshqE2e08vnglHHM_PQ&confirm=t&uuid=954a9455-3c98-4fa4-b505-487f95ec7185\n",
            "To: /content/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset/data_avignon/MNE_new_extrait.mat\n",
            " 36%|███▌      | 25.2M/70.3M [00:00<00:00, 55.9MB/s]"
          ]
        }
      ],
      "source": [
        "# Download Avignon Data\n",
        "avignon_dir = \"data_avignon\"\n",
        "os.makedirs(avignon_dir, exist_ok=True)\n",
        "ortho_id = \"1OXK3XNdrirwvnwI5yZI2AKlh3qSiSeDd\"\n",
        "mne_id = \"1iB6RllC9augWlAshqE2e08vnglHHM_PQ\"\n",
        "ortho_path = os.path.join(avignon_dir, \"Ortho_new_extrait.tif\")\n",
        "mne_path = os.path.join(avignon_dir, \"MNE_new_extrait.mat\")\n",
        "\n",
        "if not os.path.exists(ortho_path):\n",
        "    gdown.download(id=ortho_id, output=ortho_path, quiet=False)\n",
        "if not os.path.exists(mne_path):\n",
        "    gdown.download(id=mne_id, output=mne_path, quiet=False)\n",
        "print(\"Avignon data downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxoOVHRdx44A"
      },
      "outputs": [],
      "source": [
        "# Helper function to load .mat files\n",
        "def load_mat_2d(path, key=None, dtype=np.float32, squeeze=True):\n",
        "    path = Path(path)\n",
        "    try:\n",
        "        from scipy.io import loadmat\n",
        "        mdict = loadmat(path)\n",
        "        user_keys = [k for k in mdict.keys() if not k.startswith('__')]\n",
        "        if key is None: key = user_keys[0]\n",
        "        arr = mdict[key]\n",
        "    except NotImplementedError:\n",
        "        import h5py\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            available = list(f.keys())\n",
        "            if key is None: key = available[0]\n",
        "            arr = np.array(f[key])\n",
        "    if squeeze: arr = np.squeeze(arr)\n",
        "    return np.asarray(arr, dtype=dtype)\n",
        "\n",
        "# Load and Preprocess\n",
        "IMG_PATH = ortho_path\n",
        "img = io.imread(IMG_PATH)\n",
        "if img.ndim == 2: img_gray = img_as_float32(img)\n",
        "else: img_gray = img_as_float32(color.rgb2gray(img))\n",
        "\n",
        "Scale_Z = 100.0\n",
        "F = 5 # Downsampling factor for speed\n",
        "depth_raw = load_mat_2d(mne_path, key=\"mne\")\n",
        "depth_map = Scale_Z / F * depth_raw\n",
        "\n",
        "# Resize\n",
        "if img_gray.shape != depth_map.shape:\n",
        "    depth_map = resize(depth_map, img_gray.shape, order=1, preserve_range=True)\n",
        "new_shape = (img_gray.shape[0] // F, img_gray.shape[1] // F)\n",
        "img_gray_small = resize(img_gray, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "depth_map_small = resize(depth_map, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "\n",
        "print(f\"Processed shapes: Image {img_gray_small.shape}, Depth {depth_map_small.shape}\")\n",
        "\n",
        "# Visualize Inputs\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img_gray_small, cmap='gray'); ax[0].set_title(\"Intensity\")\n",
        "ax[1].imshow(img_gray_small, cmap='gray')\n",
        "im = ax[1].imshow(depth_map_small, cmap='seismic', alpha=0.6)\n",
        "ax[1].set_title(\"Intensity + Depth\")\n",
        "plt.colorbar(im, ax=ax[1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwAtqP4cx44A"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(img_input, modality_name, custom_hessian=None):\n",
        "    # 1. Hessian\n",
        "    if custom_hessian is None:\n",
        "        hessians = compute_hessians_per_scale(img_input, Σ)\n",
        "        mods = {modality_name: hessians}\n",
        "        weights = {modality_name: 1.0}\n",
        "        fused_H = fuse_hessians_per_scale(mods, weights)\n",
        "    else:\n",
        "        fused_H = custom_hessian\n",
        "\n",
        "    # 2. Graph\n",
        "    coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "    D = distances_from_similarity(S, mode=\"minus\")\n",
        "    if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "\n",
        "    # 3. Extraction\n",
        "    D_cc, idx_nodes = largest_connected_component(D)\n",
        "    mask = np.zeros_like(img_input)\n",
        "    if D_cc.shape[0] > 0:\n",
        "        Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "        labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "        sub_coords = coords[idx_nodes]\n",
        "        all_edges = []\n",
        "        for lab in np.unique(labels):\n",
        "            if lab < 0: continue\n",
        "            cl = np.where(labels == lab)[0]\n",
        "            if cl.size < 3: continue\n",
        "            mst = mst_on_cluster(D_cc, cl)\n",
        "            global_indices = idx_nodes[cl]\n",
        "            S_cluster = S[global_indices, :][:, global_indices]\n",
        "            nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "            segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "            if segs.shape[0] > 0: all_edges.append(segs)\n",
        "        if all_edges:\n",
        "            fault_edges = np.vstack(all_edges)\n",
        "            for e in fault_edges:\n",
        "                r0, c0, r1, c1, _ = e\n",
        "                rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                mask[rr, cc] = 1.0\n",
        "    return fused_H, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvFRu9e4x44B"
      },
      "outputs": [],
      "source": [
        "print(\"Processing Intensity...\")\n",
        "H_int, mask_int = run_pipeline(img_gray_small, \"intensity\")\n",
        "\n",
        "print(\"Processing Depth...\")\n",
        "H_depth, mask_depth = run_pipeline(depth_map_small, \"depth\")\n",
        "\n",
        "print(\"Processing Fusion...\")\n",
        "mods_fusion = {\n",
        "    \"intensity\": compute_hessians_per_scale(img_gray_small, Σ),\n",
        "    \"depth\": compute_hessians_per_scale(depth_map_small, Σ)\n",
        "}\n",
        "weights_fusion = {\"intensity\": 0.66, \"depth\": 0.33}\n",
        "fused_H_final = fuse_hessians_per_scale(mods_fusion, weights_fusion)\n",
        "_, mask_fusion = run_pipeline(img_gray_small, \"fused\", custom_hessian=fused_H_final)\n",
        "\n",
        "# Visualize\n",
        "def get_best_lambda2(hess_list):\n",
        "    e2n_stack = np.stack([Hd['e2n'] for Hd in hess_list], axis=0)\n",
        "    abs_e2n_stack = np.abs(e2n_stack)\n",
        "    best_idx = abs_e2n_stack.argmax(axis=0)\n",
        "    return np.take_along_axis(e2n_stack, best_idx[None,...], axis=0)[0]\n",
        "\n",
        "l2_int = get_best_lambda2(H_int)\n",
        "l2_depth = get_best_lambda2(H_depth)\n",
        "l2_fusion = get_best_lambda2(fused_H_final)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes[0,0].imshow(np.clip(l2_int,0,1), cmap='magma'); axes[0,0].set_title(\"Intensity |λ2|\")\n",
        "axes[0,1].imshow(np.clip(l2_depth,0,1), cmap='magma'); axes[0,1].set_title(\"Depth |λ2|\")\n",
        "axes[0,2].imshow(np.clip(l2_fusion,0,1), cmap='magma'); axes[0,2].set_title(\"Fusion |λ2|\")\n",
        "axes[1,0].imshow(mask_int, cmap='gray'); axes[1,0].set_title(\"Result Int\")\n",
        "axes[1,1].imshow(mask_depth, cmap='gray'); axes[1,1].set_title(\"Result Depth\")\n",
        "axes[1,2].imshow(mask_fusion, cmap='Reds'); axes[1,2].set_title(\"Result Fusion\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzE76OM7x44C"
      },
      "source": [
        "## Part 2: FIND Dataset Benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_ENHluWMgx0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GRKaM3GMgx0"
      },
      "outputs": [],
      "source": [
        "# Download FIND Dataset if not present\n",
        "import zipfile\n",
        "url = \"https://drive.google.com/uc?id=1qnLMCeon7LJjT9H0ENiNF5sFs-F7-NvK\"\n",
        "zip_path = \"data.zip\"\n",
        "extract_dir = \"data_find\"\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    if not os.path.exists(zip_path):\n",
        "        gdown.download(url, zip_path, quiet=False)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Unzipped FIND dataset.\")\n",
        "else:\n",
        "    print(\"FIND dataset already present.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHONQnmMgx0"
      },
      "outputs": [],
      "source": [
        "# --- Single Example Analysis (Seed=1) ---\n",
        "import hdbscan\n",
        "\n",
        "seed = 1\n",
        "\n",
        "print(f\"Analyzing image index {seed}...\")\n",
        "struct = auto_discover_find_structure(\"data_find\")\n",
        "dat = load_modalities_and_gt_by_index(struct, seed)\n",
        "base = dat[\"arrays\"].get(\"intensity\", next(iter(dat[\"arrays\"].values())))\n",
        "gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "\n",
        "# 1. Compute Hessians & Fusion\n",
        "mods_hess = {}\n",
        "weights = {\"intensity\": 0.4, \"range\": 0.1, \"filtered\": 0.5, \"fused\": 0.0}\n",
        "valid_keys = [k for k in weights if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "for k in valid_keys: mods_hess[k] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][k]), Σ)\n",
        "fused_H = fuse_hessians_per_scale(mods_hess, weights)\n",
        "\n",
        "# 2. Graph Construction\n",
        "coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "D = distances_from_similarity(S, mode=\"minus\")\n",
        "if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "print(f\"Largest CC: {D_cc.shape[0]} nodes.\")\n",
        "\n",
        "# 3. HDBSCAN Clustering\n",
        "sub_coords = coords[idx_nodes]\n",
        "if D_cc.shape[0] > 0:\n",
        "    Dist = D_cc.copy()\n",
        "    Dist.data = Dist.data ** expZ\n",
        "    Dist = Dist.tocsr()\n",
        "    Dist.setdiag(0.0)\n",
        "\n",
        "    # Explicit HDBSCAN call as requested\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=\"precomputed\",\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        max_dist=max_dist,\n",
        "        allow_single_cluster=allow_single_cluster,\n",
        "    )\n",
        "    labels = clusterer.fit_predict(Dist)\n",
        "\n",
        "    # Detailed output\n",
        "    print(\"Clusters:\", np.unique(labels), \". 'Noise':\", np.sum(labels == -1))\n",
        "else:\n",
        "    labels = np.array([])\n",
        "\n",
        "# 4. Skeletonization (MST + Betweenness)\n",
        "all_edges = []\n",
        "sk_pred_mask = np.zeros_like(base, dtype=np.uint8)\n",
        "if labels.size > 0:\n",
        "    for lab in np.unique(labels):\n",
        "        if lab < 0: continue\n",
        "        cl = np.where(labels == lab)[0]\n",
        "        if cl.size < 3: continue\n",
        "        mst = mst_on_cluster(D_cc, cl)\n",
        "        global_indices = idx_nodes[cl]\n",
        "        S_cluster = S[global_indices, :][:, global_indices]\n",
        "        nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "        segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "        if segs.shape[0] > 0: all_edges.append(segs)\n",
        "    if all_edges:\n",
        "        fault_edges = np.vstack(all_edges)\n",
        "        for e in fault_edges:\n",
        "            r0, c0, r1, c1, _ = e\n",
        "            rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "            rr, cc = np.clip(rr.astype(int), 0, sk_pred_mask.shape[0]-1), np.clip(cc.astype(int), 0, sk_pred_mask.shape[1]-1)\n",
        "            sk_pred_mask[rr, cc] = 1\n",
        "\n",
        "# 5. Visualizations\n",
        "l2_stack = np.stack([Hd['e2n'] for Hd in fused_H], axis=0)\n",
        "l2_fused_vis = np.max(np.abs(l2_stack), axis=0)\n",
        "sim_vis = np.zeros_like(base)\n",
        "if S.shape[0] > 0:\n",
        "    degs = np.array(S.sum(axis=1)).flatten()\n",
        "    sim_vis[coords[:,0], coords[:,1]] = degs\n",
        "\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Row 1\n",
        "plt.subplot(3, 4, 1); plt.title(\"Intensity\"); plt.imshow(dat[\"arrays\"][\"intensity\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "plt.subplot(3, 4, 2); plt.title(\"Range\"); plt.imshow(dat[\"arrays\"][\"range\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "if 'filtered' in dat['arrays']:\n",
        "    plt.subplot(3, 4, 3); plt.title(\"Filtered Range\"); plt.imshow(dat[\"arrays\"][\"filtered\"], cmap=\"gray\"); plt.axis(\"off\")\n",
        "else:\n",
        "    plt.subplot(3, 4, 3); plt.axis(\"off\")\n",
        "plt.subplot(3, 4, 4); plt.title(\"Fused |λ2|\"); plt.imshow(l2_fused_vis, cmap=\"magma\"); plt.axis(\"off\")\n",
        "\n",
        "# Row 2\n",
        "plt.subplot(3, 4, 5); plt.title(\"Frangi Similarity (Degree)\"); plt.imshow(sim_vis, cmap=\"inferno\"); plt.axis(\"off\")\n",
        "ax_cl = plt.subplot(3, 4, 6); ax_cl.set_title(\"HDBSCAN Clusters\")\n",
        "ax_cl.imshow(base, cmap='gray', alpha=0.5)\n",
        "if labels.size > 0:\n",
        "    # Separate noise and clusters for visualization\n",
        "    noise_mask = labels == -1\n",
        "    cluster_mask = labels >= 0\n",
        "    if np.any(noise_mask):\n",
        "        ax_cl.scatter(sub_coords[noise_mask, 1], sub_coords[noise_mask, 0], c='k', s=0.5, alpha=0.3, label='Noise')\n",
        "    if np.any(cluster_mask):\n",
        "        ax_cl.scatter(sub_coords[cluster_mask, 1], sub_coords[cluster_mask, 0], c=labels[cluster_mask], cmap='tab20', s=1)\n",
        "ax_cl.axis('off')\n",
        "\n",
        "# Ours Skeleton\n",
        "ax_sk = plt.subplot(3, 4, 7); ax_sk.set_title(\"Ours: Skeleton Overlay\")\n",
        "ax_sk.imshow(base, cmap='gray')\n",
        "ax_sk.imshow(sk_pred_mask, cmap='Reds', alpha=0.7, vmin=0, vmax=1)\n",
        "ax_sk.axis('off')\n",
        "\n",
        "# Ground Truth\n",
        "plt.subplot(3, 4, 8); plt.title(\"Ground Truth\"); plt.imshow(gt, cmap=\"gray\"); plt.axis(\"off\")\n",
        "\n",
        "# Row 3\n",
        "csd_path = f\"/content/drive/MyDrive/Datasets/FIND/Results/CrackSegDiff/20000_1000/test_output_fused/im{seed+1:05d}_output_ens.png\"\n",
        "if os.path.exists(csd_path):\n",
        "    try:\n",
        "        csd_img = np.array(Image.open(csd_path).convert('L'))\n",
        "        csd_bin = (csd_img > 127).astype(np.uint8)\n",
        "        plt.subplot(3, 4, 9); plt.title(\"CrackSegDiff\"); plt.imshow(csd_bin, cmap=\"Greens\"); plt.axis(\"off\")\n",
        "        ax_ov = plt.subplot(3, 4, 10); ax_ov.set_title(\"Overlay: GT(W)+Ours(R)+CSD(G)\")\n",
        "        ax_ov.imshow(np.zeros_like(base), cmap='gray')\n",
        "        ax_ov.imshow(gt, cmap='Greys', alpha=0.5)\n",
        "        ax_ov.imshow(sk_pred_mask, cmap='Reds', alpha=0.5)\n",
        "        ax_ov.imshow(csd_bin, cmap='Greens', alpha=0.3)\n",
        "        ax_ov.axis('off')\n",
        "    except Exception as e: print(\"Error loading CSD:\", e)\n",
        "else:\n",
        "    print(\"CrackSegDiff result not found at:\", csd_path)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Metrics for this image\n",
        "sk_pred_thick = thicken(sk_pred_mask, pixels=3)\n",
        "sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)\n",
        "print(\"--- Metrics (Seed 1) ---\")\n",
        "print(\"Jaccard:\", jaccard_index(sk_pred_thick, sk_gt_thick))\n",
        "print(\"Tversky:\", tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5))\n",
        "print(\"Wasserstein:\", wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duPkt3E1Mgx1"
      },
      "outputs": [],
      "source": [
        "# --- Batch Processing 500 Images (USE_COMBO=True) ---\n",
        "import itertools\n",
        "\n",
        "excluded_ids = []\n",
        "start_idx = 0\n",
        "end_idx = 500\n",
        "n_jobs = 8\n",
        "USE_COMBO = True\n",
        "allow_single_cluster = True\n",
        "output_dir = \"/content/drive/MyDrive/Datasets/FIND/Results/Avignon_Notebook_Batch\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Results will be saved to: {output_dir}\")\n",
        "\n",
        "def process_image_idx_combo(idx):\n",
        "    if idx in excluded_ids: return None\n",
        "    try:\n",
        "        dat = load_modalities_and_gt_by_index(struct, idx)\n",
        "        base = dat[\"arrays\"].get(\"intensity\", next(iter(dat[\"arrays\"].values())))\n",
        "        gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "        gt = binary_closing(gt, footprint=disk(2))\n",
        "        gt = binary_opening(gt, footprint=disk(2))\n",
        "        sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)\n",
        "\n",
        "        # Pre-compute Hessians (Normal & Inverted)\n",
        "        hessian_cache = {}\n",
        "        valid_keys = [k for k in weights if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "        for k in valid_keys:\n",
        "            arr = to_gray(dat[\"arrays\"][k])\n",
        "            h_norm = compute_hessians_per_scale(arr, Σ)\n",
        "            hessian_cache[k] = [h_norm]\n",
        "            if USE_COMBO:\n",
        "                h_inv = compute_hessians_per_scale(255 - arr, Σ)\n",
        "                hessian_cache[k].append(h_inv)\n",
        "\n",
        "        # Combinations\n",
        "        combo_indices = list(itertools.product([0, 1], repeat=len(valid_keys))) if USE_COMBO else [tuple(0 for _ in valid_keys)]\n",
        "\n",
        "        best_tversky = -1.0\n",
        "        best_res = None\n",
        "\n",
        "        for combo in combo_indices:\n",
        "            current_mods = {}\n",
        "            for i, mod in enumerate(valid_keys):\n",
        "                current_mods[mod] = hessian_cache[mod][combo[i]]\n",
        "\n",
        "            fused_H = fuse_hessians_per_scale(current_mods, weights)\n",
        "            coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges)\n",
        "            D = distances_from_similarity(S, mode=\"minus\")\n",
        "            if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "            D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "            sk_pred = np.zeros_like(base, dtype=np.uint8)\n",
        "            if D_cc.shape[0] > 0:\n",
        "                expZ = 1\n",
        "                Dist = D_cc.copy()\n",
        "                Dist.data = Dist.data ** expZ\n",
        "                Dist = Dist.tocsr()\n",
        "                Dist.setdiag(0.0)\n",
        "\n",
        "                # Explicit HDBSCAN call\n",
        "                clusterer = hdbscan.HDBSCAN(\n",
        "                    metric=\"precomputed\",\n",
        "                    min_cluster_size=min_cluster_size,\n",
        "                    min_samples=min_samples,\n",
        "                    max_dist=max_dist,\n",
        "                    allow_single_cluster=allow_single_cluster,\n",
        "                )\n",
        "                labels = clusterer.fit_predict(Dist)\n",
        "\n",
        "                sub_coords = coords[idx_nodes]\n",
        "                all_edges = []\n",
        "                for lab in np.unique(labels):\n",
        "                    if lab < 0: continue\n",
        "                    cl = np.where(labels == lab)[0]\n",
        "                    if cl.size < 3: continue\n",
        "                    mst = mst_on_cluster(D_cc, cl)\n",
        "                    global_indices = idx_nodes[cl]\n",
        "                    S_cluster = S[global_indices, :][:, global_indices]\n",
        "                    nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "                    segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "                    if segs.shape[0] > 0: all_edges.append(segs)\n",
        "                if all_edges:\n",
        "                    fault_edges = np.vstack(all_edges)\n",
        "                    mask = np.zeros_like(base, dtype=np.uint8)\n",
        "                    for e in fault_edges:\n",
        "                        r0, c0, r1, c1, _ = e\n",
        "                        rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                        rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                        mask[rr, cc] = 1\n",
        "                    sk_pred = skeletonize_lee(mask)\n",
        "\n",
        "            sk_pred_thick = thicken(sk_pred, pixels=3)\n",
        "            tvs = tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5)\n",
        "\n",
        "            if tvs > best_tversky:\n",
        "                best_tversky = tvs\n",
        "                jac = jaccard_index(sk_pred_thick, sk_gt_thick)\n",
        "                wass = wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)\n",
        "                best_res = {\n",
        "                    \"Image\": f\"im{idx+1:05d}\",\n",
        "                    \"Jaccard\": jac,\n",
        "                    \"Tversky\": tvs,\n",
        "                    \"Wasserstein\": wass,\n",
        "                    \"Combo\": str(combo)\n",
        "                }\n",
        "        return best_res\n",
        "    except Exception as e: return None\n",
        "\n",
        "print(f\"Processing batch {start_idx}-{end_idx}...\")\n",
        "with tqdm_joblib(tqdm(total=end_idx-start_idx)) as progress_bar:\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_image_idx_combo)(i) for i in range(start_idx, end_idx))\n",
        "\n",
        "results = [r for r in results if r is not None]\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(\"\\n--- Results ---\")\n",
        "    print(df_res[[\"Jaccard\", \"Tversky\", \"Wasserstein\"]].mean())\n",
        "    df_res.to_csv(os.path.join(output_dir, \"metrics_combo.csv\"), index=False)\n",
        "else: print(\"No valid results.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}