{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Universal and Robust Multi-Modal Crack Extraction via Generalized Frangi Graphs and Topological Centrality\n",
        "\n",
        "**Abstract**\n",
        "Automatic crack detection is a pivotal task in structural health monitoring and geoscience. We propose a \"universal\", training-free approach that robustly extracts crack networks across varying data distributions. Our method generalizes the classical Frangi vesselness filter to the multi-modal setting, fusing photometric (intensity) and geometric (range/depth) data at the Hessian level. Instead of pixel-wise classification, we construct a sparse graph driven by a pairwise Frangi similarity metric. A novel topological extraction algorithm—combining HDBSCAN and Weighted Betweenness Centrality on a MST—isolates the precise topological skeleton.\n",
        "\n",
        "This notebook reproduces the experiments presented in the paper (ICPR 2025, Lyon), illustrating the method on:\n",
        "1.  **Real-world geological data**: The *Palais des Papes* in Avignon.\n",
        "2.  **The FIND Benchmark**: A dataset of 500 registered intensity and range images.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Environment Setup\n",
        "!pip -q install numpy scipy scikit-image matplotlib joblib tqdm tqdm-joblib hdbscan networkx gdown tifffile imageio pandas Pillow pot\n",
        "\n",
        "%%bash\n",
        "if [ ! -d \"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\" ]; then\n",
        "  git clone https://github.com/Ludwig-H/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset.git\n",
        "fi\n",
        "cd Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
        "pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Imports and Global Parameters\n",
        "import os, sys, numpy as np, matplotlib.pyplot as plt\n",
        "import gdown, imageio.v2 as iio, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "from skimage import io, color, img_as_float32\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import binary_closing, binary_opening, disk\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from joblib import Parallel, delayed\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure local src is importable\n",
        "repo_root = os.path.abspath(\"./Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.append(repo_root)\n",
        "\n",
        "from frangi_fusion import (\n",
        "    set_seed, load_modalities_and_gt_by_index, to_gray,\n",
        "    compute_hessians_per_scale, fuse_hessians_per_scale,\n",
        "    build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph,\n",
        "    largest_connected_component, hdbscan_from_sparse,\n",
        "    mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph,\n",
        "    skeletonize_lee, thicken, jaccard_index, tversky_index, wasserstein_distance_skeletons,\n",
        "    auto_discover_find_structure\n",
        ")\n",
        "\n",
        "# --- Global Hyper-parameters ---\n",
        "Σ = [1, 3, 5, 7, 9]  # Gaussian scales\n",
        "β = 0.5    # Frangi blob sensitivity\n",
        "c = 0.25   # Contrast sensitivity\n",
        "c_θ = 0.125 # Orientation sensitivity\n",
        "R = 5      # Graph neighbor radius\n",
        "K = 1      # 1 = Standard, 2 = Triangle Connectivity\n",
        "dark_ridges = True\n",
        "min_cluster_size = 512\n",
        "f_threshold = 0.25\n",
        "\n",
        "print(\"Libraries loaded and parameters defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methodology: Hessian Fusion and Generalized Frangi Graph\n",
        "\n",
        "### 1. Multi-Modal Hessian Fusion\n",
        "For each modality $m$ (e.g., Intensity, Depth) and scale $\\sigma \\in \\Sigma$, we compute the normalized Hessian matrix $\\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x})$ at pixel $\\mathbf{x}$. To handle disparate dynamic ranges, we normalize by the maximum spectral norm:\n",
        "$$ \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) = \\frac{\\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{x})}{\\max_{\\mathbf{y} \\in \\Omega} \\| \\mathcal{H}_{\\sigma}^{(m)}(\\mathbf{y}) \\|} $$\n",
        "The **Fused Hessian** is a weighted linear combination:\n",
        "$$ \\mathcal{H}_{\\sigma}^{\\text{fused}}(\\mathbf{x}) = \\sum_{m} w_m \\hat{\\mathcal{H}}_{\\sigma}^{(m)}(\\mathbf{x}) $$ \n",
        "We analyze eigenvalues $\\lambda_1, \\lambda_2$ (with $|\\lambda_1| \\le |\\lambda_2|$) and eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2$. For cracks (dark ridges), we expect $\\lambda_2 > 0$.\n",
        "\n",
        "### 2. Generalized Frangi Similarity\n",
        "We construct a graph where edges connect pixels $i, j$ within radius $R=5$. The pairwise similarity $S_{ij} ∈ [0,1]$ enforces local tubular geometry:\n",
        "\n",
        "1.  **Elongation ($S_{\\text{shape}}$):** Using ratio $\\mathcal{R}_B = |\\lambda_1| / |\\lambda_2|$:\n",
        "    $$ S_{\\text{shape}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\mathcal{R}_B(\\mathbf{x}_i) + \\mathcal{R}_B(\\mathbf{x}_j)}{\\beta}\\right)^2\\right) $$ \n",
        "2.  **Contrast ($S_{\\text{int}}$):** Using energy $\\mathcal{S} = \\|\\mathcal{H}\\|_F$:\n",
        "    $$ S_{\\text{int}} = 1 - \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{\\mathcal{S}(\\mathbf{x}_i) \\cdot \\mathcal{S}(\\mathbf{x}_j)}}{c}\\right)^2\\right) $$ \n",
        "3.  **Alignment ($S_{\\text{align}}$):** Penalizing deviation from $\\mathbf{v}_1$:\n",
        "    $$ S_{\\text{align}} = \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sin(\\delta_\\theta)}{c_\\theta}\\right)^2\\right) $$ \n",
        "\n",
        "The final distance metric is $d_{ij} = (1 - S_{ij}) \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$.\n",
        "\n",
        "### 3. Topological Extraction\n",
        "The graph is processed sequentially: **HDBSCAN Clustering**, **MST**, and **Weighted Betweenness Centrality ($C_B$)** to extract the skeleton:\n",
        "$$ C_B(v) = \\sum_{s ≠ v ≠ t} \\frac{\\eta_{st}(v)}{\\eta_{st}} $$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Case Study - Palais des Papes (Avignon)\n",
        "We demonstrate the power of fusion on a challenging real-world case: the retaining rock of the Palais des Papes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Avignon Data\n",
        "avignon_dir = \"data_avignon\"\n",
        "os.makedirs(avignon_dir, exist_ok=True)\n",
        "ortho_id = \"1OXK3XNdrirwvnwI5yZI2AKlh3qSiSeDd\"\n",
        "mne_id = \"1iB6RllC9augWlAshqE2e08vnglHHM_PQ\"\n",
        "ortho_path = os.path.join(avignon_dir, \"Ortho_new_extrait.tif\")\n",
        "mne_path = os.path.join(avignon_dir, \"MNE_new_extrait.mat\")\n",
        "\n",
        "if not os.path.exists(ortho_path):\n",
        "    gdown.download(id=ortho_id, output=ortho_path, quiet=False)\n",
        "if not os.path.exists(mne_path):\n",
        "    gdown.download(id=mne_id, output=mne_path, quiet=False)\n",
        "print(\"Avignon data downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to load .mat files\n",
        "def load_mat_2d(path, key=None, dtype=np.float32, squeeze=True):\n",
        "    path = Path(path)\n",
        "    try:\n",
        "        from scipy.io import loadmat\n",
        "        mdict = loadmat(path)\n",
        "        user_keys = [k for k in mdict.keys() if not k.startswith('__')]\n",
        "        if key is None: key = user_keys[0]\n",
        "        arr = mdict[key]\n",
        "    except NotImplementedError:\n",
        "        import h5py\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            available = list(f.keys())\n",
        "            if key is None: key = available[0]\n",
        "            arr = np.array(f[key])\n",
        "    if squeeze: arr = np.squeeze(arr)\n",
        "    return np.asarray(arr, dtype=dtype)\n",
        "\n",
        "# Load and Preprocess\n",
        "IMG_PATH = ortho_path\n",
        "img = io.imread(IMG_PATH)\n",
        "if img.ndim == 2: img_gray = img_as_float32(img)\n",
        "else: img_gray = img_as_float32(color.rgb2gray(img))\n",
        "\n",
        "Scale_Z = 100.0\n",
        "F = 2\n",
        "depth_raw = load_mat_2d(mne_path, key=\"mne\")\n",
        "depth_map = Scale_Z / F * depth_raw\n",
        "\n",
        "# Resize\n",
        "if img_gray.shape != depth_map.shape:\n",
        "    depth_map = resize(depth_map, img_gray.shape, order=1, preserve_range=True)\n",
        "new_shape = (img_gray.shape[0] // F, img_gray.shape[1] // F)\n",
        "img_gray_small = resize(img_gray, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "depth_map_small = resize(depth_map, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "\n",
        "print(f\"Processed shapes: Image {img_gray_small.shape}, Depth {depth_map_small.shape}\")\n",
        "\n",
        "# Visualize Inputs\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img_gray_small, cmap='gray'); ax[0].set_title(\"Intensity\")\n",
        "ax[1].imshow(img_gray_small, cmap='gray')\n",
        "im = ax[1].imshow(depth_map_small, cmap='seismic', alpha=0.6)\n",
        "ax[1].set_title(\"Intensity + Depth\")\n",
        "plt.colorbar(im, ax=ax[1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_pipeline(img_input, modality_name, custom_hessian=None):\n",
        "    # 1. Hessian\n",
        "    if custom_hessian is None:\n",
        "        hessians = compute_hessians_per_scale(img_input, Σ)\n",
        "        mods = {modality_name: hessians}\n",
        "        weights = {modality_name: 1.0}\n",
        "        fused_H = fuse_hessians_per_scale(mods, weights)\n",
        "    else:\n",
        "        fused_H = custom_hessian\n",
        "\n",
        "    # 2. Graph\n",
        "    coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, dark_ridges=dark_ridges)\n",
        "    D = distances_from_similarity(S, mode=\"minus\")\n",
        "    if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "\n",
        "    # 3. Extraction\n",
        "    D_cc, idx_nodes = largest_connected_component(D)\n",
        "    mask = np.zeros_like(img_input)\n",
        "    if D_cc.shape[0] > 0:\n",
        "        Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "        labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "        sub_coords = coords[idx_nodes]\n",
        "        all_edges = []\n",
        "        for lab in np.unique(labels):\n",
        "            if lab < 0: continue\n",
        "            cl = np.where(labels == lab)[0]\n",
        "            if cl.size < 3: continue\n",
        "            mst = mst_on_cluster(D_cc, cl)\n",
        "            global_indices = idx_nodes[cl]\n",
        "            S_cluster = S[global_indices, :][:, global_indices]\n",
        "            nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "            segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "            if segs.shape[0] > 0: all_edges.append(segs)\n",
        "        if all_edges:\n",
        "            fault_edges = np.vstack(all_edges)\n",
        "            for e in fault_edges:\n",
        "                r0, c0, r1, c1, _ = e\n",
        "                rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                mask[rr, cc] = 1.0\n",
        "    return fused_H, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Processing Intensity...\")\n",
        "H_int, mask_int = run_pipeline(img_gray_small, \"intensity\")\n",
        "\n",
        "print(\"Processing Depth...\")\n",
        "H_depth, mask_depth = run_pipeline(depth_map_small, \"depth\")\n",
        "\n",
        "print(\"Processing Fusion...\")\n",
        "mods_fusion = {\n",
        "    \"intensity\": compute_hessians_per_scale(img_gray_small, Σ),\n",
        "    \"depth\": compute_hessians_per_scale(depth_map_small, Σ)\n",
        "}\n",
        "weights_fusion = {\"intensity\": 0.66, \"depth\": 0.33}\n",
        "fused_H_final = fuse_hessians_per_scale(mods_fusion, weights_fusion)\n",
        "_, mask_fusion = run_pipeline(img_gray_small, \"fused\", custom_hessian=fused_H_final)\n",
        "\n",
        "# Visualize\n",
        "def get_best_lambda2(hess_list):\n",
        "    e2n_stack = np.stack([Hd['e2n'] for Hd in hess_list], axis=0)\n",
        "    abs_e2n_stack = np.abs(e2n_stack)\n",
        "    best_idx = abs_e2n_stack.argmax(axis=0)\n",
        "    return np.take_along_axis(e2n_stack, best_idx[None,...], axis=0)[0]\n",
        "\n",
        "l2_int = get_best_lambda2(H_int)\n",
        "l2_depth = get_best_lambda2(H_depth)\n",
        "l2_fusion = get_best_lambda2(fused_H_final)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes[0,0].imshow(np.clip(l2_int,0,1), cmap='magma'); axes[0,0].set_title(\"Intensity |λ2|\")\n",
        "axes[0,1].imshow(np.clip(l2_depth,0,1), cmap='magma'); axes[0,1].set_title(\"Depth |λ2|\")\n",
        "axes[0,2].imshow(np.clip(l2_fusion,0,1), cmap='magma'); axes[0,2].set_title(\"Fusion |λ2|\")\n",
        "axes[1,0].imshow(mask_int, cmap='gray'); axes[1,0].set_title(\"Result Int\")\n",
        "axes[1,1].imshow(mask_depth, cmap='gray'); axes[1,1].set_title(\"Result Depth\")\n",
        "axes[1,2].imshow(mask_fusion, cmap='Reds'); axes[1,2].set_title(\"Result Fusion\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: FIND Dataset Benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "url = \"https://drive.google.com/uc?id=1qnLMCeon7LJjT9H0ENiNF5sFs-F7-NvK\"\n",
        "zip_path = \"data.zip\"\n",
        "extract_dir = \"data_find\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "if not os.path.exists(zip_path):\n",
        "    gdown.download(url, zip_path, quiet=False)\n",
        "if not os.path.exists(os.path.join(extract_dir, \"00001.png\")):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Unzipped FIND dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 1\n",
        "struct = auto_discover_find_structure(\"data_find\")\n",
        "dat = load_modalities_and_gt_by_index(struct, seed)\n",
        "print(\"Sample:\", seed)\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (k, arr) in enumerate(dat[\"arrays\"].items()):\n",
        "    plt.subplot(1, len(dat[\"arrays\"].items()), i+1)\n",
        "    plt.title(k); plt.imshow(arr, cmap='gray'); plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "excluded_ids = []\n",
        "start_idx = 0\n",
        "end_idx = 500\n",
        "n_jobs = 8\n",
        "weights = {\"intensity\": 0.4, \"range\": 0.1, \"filtered\": 0.5, \"fused\": 0.0}\n",
        "output_dir = \"Results_FIND\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def process_image_idx(idx):\n",
        "    if idx in excluded_ids: return None\n",
        "    try:\n",
        "        dat = load_modalities_and_gt_by_index(struct, idx)\n",
        "        base = dat[\"arrays\"].get(\"intensity\", next(iter(dat['arrays'].values())))\n",
        "        img_name = f\"im{idx+1:05d}\")\n",
        "        gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "        gt = binary_closing(gt, footprint=disk(2))\n",
        "        gt = binary_opening(gt, footprint=disk(2))\n",
        "        sk_gt_thick = thicken(skeletonize_lee(gt), pixels=3)\n",
        "\n",
        "        mods_hess = {}\n",
        "        valid_keys = [k for k in weights if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "        for k in valid_keys: mods_hess[k] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][k]), Σ)\n",
        "        fused_H = fuse_hessians_per_scale(mods_hess, weights)\n",
        "\n",
        "        coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, dark_ridges=dark_ridges)\n",
        "        D = distances_from_similarity(S, mode=\"minus\")\n",
        "        if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "        D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "        sk_pred = np.zeros_like(base, dtype=np.uint8)\n",
        "        if D_cc.shape[0] > 0:\n",
        "            Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "            labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "            sub_coords = coords[idx_nodes]\n",
        "            all_fault_edges = []\n",
        "            for lab in np.unique(labels):\n",
        "                if lab < 0: continue\n",
        "                cl = np.where(labels == lab)[0]\n",
        "                if cl.size < 3: continue\n",
        "                mst = mst_on_cluster(D_cc, cl)\n",
        "                global_indices = idx_nodes[cl]\n",
        "                S_cluster = S[global_indices, :][:, global_indices]\n",
        "                nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=0.2, S=S_cluster, take_similarity=True)\n",
        "                segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "                if segs.shape[0] > 0: all_fault_edges.append(segs)\n",
        "            if all_fault_edges:\n",
        "                fault_edges = np.vstack(all_fault_edges)\n",
        "                mask = np.zeros_like(base, dtype=np.uint8)\n",
        "                for e in fault_edges:\n",
        "                    r0, c0, r1, c1, _ = e\n",
        "                    rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                    rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                    mask[rr.astype(int), cc.astype(int)] = 1\n",
        "                sk_pred = skeletonize_lee(mask)\n",
        "\n",
        "        sk_pred_thick = thicken(sk_pred, pixels=3)\n",
        "        return {\n",
        "            \"Image\": img_name,\n",
        "            \"Jaccard\": jaccard_index(sk_pred_thick, sk_gt_thick),\n",
        "            \"Tversky\": tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5),\n",
        "            \"Wasserstein\": wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)\n",
        "        }\n",
        "    except Exception as e: return None\n",
        "\n",
        "print(f\"Batch processing {start_idx}-{end_idx}...\")\n",
        "with tqdm_joblib(tqdm(total=end_idx-start_idx)) as progress_bar:\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_image_idx)(i) for i in range(start_idx, end_idx))\n",
        "\n",
        "results = [r for r in results if r is not None]\n",
        "df_res = pd.DataFrame(results)\n",
        "if not df_res.empty:\n",
        "    print(\"\\n--- Results ---\")\n",
        "    print(df_res[[\"Jaccard\", \"Tversky\", \"Wasserstein\"]].mean())\n",
        "    df_res.to_csv(os.path.join(output_dir, \"metrics.csv\"), index=False)\n",
        "else: print(\"No valid results.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}