{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# Universal and Robust Multi-Modal Crack Extraction via Generalized Frangi Graphs and Topological Centrality\n",
        "\n",
        "**Abstract**\n",
        "Automatic crack detection is a pivotal task in structural health monitoring and geoscience. We propose a \"universal\", training-free approach that robustly extracts crack networks across varying data distributions (e.g., concrete, rock, pavement). Our method generalizes the classical Frangi vesselness filter to the multi-modal setting, fusing photometric (intensity) and geometric (range/depth) data at the Hessian level. Instead of pixel-wise classification, we construct a sparse graph driven by a pairwise Frangi similarity metric, which rigorously encodes local tubular geometry. A novel topological extraction algorithm—combining Hierarchical Density-Based Spatial Clustering (HDBSCAN) and Weighted Betweenness Centrality on a Minimum Spanning Tree (MST)—isolates the precise topological skeleton of the faults.\n",
        "\n",
        "This notebook reproduces the experiments presented in the paper (ICPR 2025, Lyon), illustrating the method on:\n",
        "1.  **Real-world geological data**: The *Palais des Papes* in Avignon (Orthophoto + Photogrammetric Depth).\n",
        "2.  **The FIND Benchmark**: A dataset of 500 registered intensity and range images with ground truth.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_env"
      },
      "outputs": [],
      "source": [
        "# 1. Environment Setup\n",
        "# We install necessary dependencies and clone the repository containing the source code.\n",
        "\n",
        "!pip -q install numpy scipy scikit-image matplotlib joblib tqdm tqdm-joblib hdbscan networkx gdown tifffile imageio pandas Pillow pot\n",
        "\n",
        "%%bash\n",
        "# Clone the repository if not already present\n",
        "if [ ! -d \"Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\" ]; then\n",
        "  git clone https://github.com/Ludwig-H/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset.git\n",
        "fi\n",
        "cd Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
        "pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_params"
      },
      "outputs": [],
      "source": [
        "# 2. Imports and Global Parameters\n",
        "\n",
        "import os, sys, numpy as np, matplotlib.pyplot as plt\n",
        "import gdown, imageio.v2 as iio, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "\n",
        "from skimage import io, color, img_as_float32\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import binary_closing, binary_opening, disk\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from joblib import Parallel, delayed\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure local src is importable\n",
        "repo_root = os.path.abspath(\"./Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.append(repo_root)\n",
        "\n",
        "from frangi_fusion import (\n",
        "    set_seed, load_modalities_and_gt_by_index, to_gray,\n",
        "    compute_hessians_per_scale, fuse_hessians_per_scale,\n",
        "    build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph,\n",
        "    largest_connected_component, hdbscan_from_sparse,\n",
        "    mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph,\n",
        "    overlay_hessian_orientation, show_clusters_on_image,\n",
        "    skeletonize_lee, thicken, jaccard_index, tversky_index, wasserstein_distance_skeletons,\n",
        "    auto_discover_find_structure\n",
        ")\n",
        "\n",
        "# --- Global Hyper-parameters --- \n",
        "# These parameters are fixed for all experiments to demonstrate robustness.\n",
        "\n",
        "# Multi-scale analysis\n",
        "Σ = [1, 3, 5, 7, 9]  # Gaussian scales (sigma)\n",
        "\n",
        "# Frangi Similarity Parameters\n",
        "β = 0.5    # Sensitivity to blob-like structures (Frangi ratio)\n",
        "c = 0.25   # Sensitivity to structure contrast (Frobenius norm)\n",
        "c_θ = 0.125 # Sensitivity to orientation alignment\n",
        "\n",
        "# Graph Construction\n",
        "R = 5      # Neighborhood radius (pixels)\n",
        "K = 1      # 1 = Standard Frangi Distance, 2 = Triangle Connectivity (Rips)\n",
        "dark_ridges = True # Target dark cracks on bright background (valleys)\n",
        "\n",
        "# Clustering & Topological Extraction\n",
        "min_cluster_size = 512 # HDBSCAN parameter\n",
        "f_threshold = 0.25     # Betweenness centrality pruning threshold\n",
        "\n",
        "print(\"Libraries loaded and parameters defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "math_intro"
      },
      "source": [
        "## Methodology: Hessian Fusion and Generalized Frangi Graph\n",
        "\n",
        "### 1. Multi-Modal Hessian Fusion\n",
        "For each modality $m$ (e.g., Intensity, Depth) and scale $\sigma \in \Sigma$, we compute the normalized Hessian matrix $\hat{\mathcal{H}}_{\sigma}^{(m)}(\mathbf{x})$ at pixel $\mathbf{x}$. To handle disparate dynamic ranges, we normalize by the maximum spectral norm:\n",
        "$$\n",
        "\hat{\mathcal{H}}_{\sigma}^{(m)}(\mathbf{x}) = \frac{\mathcal{H}_{\sigma}^{(m)}(\mathbf{x})}{\max_{\mathbf{y} \in \Omega} \| \mathcal{H}_{\sigma}^{(m)}(\mathbf{y}) \|}
",
        "$$\n",
        "The **Fused Hessian** is a weighted linear combination that exploits the constructive interference of eigenvectors (curvature direction) across modalities:\n",
        "$$\n",
        "\mathcal{H}_{\sigma}^{\text{fused}}(\mathbf{x}) = \sum_{m} w_m \hat{\mathcal{H}}_{\sigma}^{(m)}(\mathbf{x})
",
        "$$\n",
        "We analyze the eigen-decomposition of $\mathcal{H}_{\sigma}^{\text{fused}}$: eigenvalues $$\lambda_1, \lambda_2$$ (with $|\lambda_1| \le |\lambda_2|$) and eigenvectors $$\mathbf{v}_1, \mathbf{v}_2$$. For cracks (dark ridges), we expect $$\lambda_2 > 0$.\n",
        "\n",
        "### 2. Generalized Frangi Similarity\n",
        "We construct a graph where edges connect pixels $i, j$ within radius $R=5$. The pairwise similarity $S_{ij} \in [0,1]$ enforces local tubular geometry:\n",
        "\n",
        "1.  **Elongation ($S_{\text{shape}}$):** Ensures the structure is tubular (not blob-like) using the ratio $$\mathcal{R}_B = |\lambda_1| / |\lambda_2|$$.\n",
        "    $$ S_{\text{shape}} = \exp\left(-\frac{1}{2} \left(\frac{\mathcal{R}_B(\mathbf{x}_i) + \mathcal{R}_B(\mathbf{x}_j)}{\beta}\right)^2\right) $\n",
        "2.  **Contrast ($S_{\text{int}}$):** Favors regions with high second-order derivative energy $$\mathcal{S} = \|\mathcal{H}\|_F$$.\n",
        "    $$ S_{\text{int}} = 1 - \exp\left(-\frac{1}{2} \left(\frac{\sqrt{\mathcal{S}(\mathbf{x}_i) \cdot \mathcal{S}(\mathbf{x}_j)}}{c}\right)^2\right) $\n",
        "3.  **Alignment ($S_{\text{align}}$):** Penalizes edges deviating from the estimated vessel direction $$\mathbf{v}_1$$. Let $$\delta_\theta$$ be the angle between the edge vector $$\mathbf{x}_j - \mathbf{x}_i$$ and $$\mathbf{v}_1$$.\n",
        "    $$ S_{\text{align}} = \exp\left(-\frac{1}{2} \left(\frac{\sin(\delta_\theta)}{c_\theta}\right)^2\right) $\n",
        "\n",
        "The final distance metric is $d_{ij} = (1 - S_{ij}) \|\mathbf{x}_i - \mathbf{x}_j\|$.\n",
        "\n",
        "### 3. Topological Extraction\n",
        "The graph is processed sequentially:\n",
        "*   **HDBSCAN Clustering:** Isolates coherent fault structures from noise.\n",
        "*   **MST (Minimum Spanning Tree):** Reduces cycles and simplifies topology.\n",
        "*   **Weighted Betweenness Centrality ($C_B$):** Identifies the \"backbone\" of the crack. Nodes on the geometric spine minimize travel cost and accumulate high centrality.\n",
        "    $$ C_B(v) = \sum_{s \neq v \neq t} \frac{\eta_{st}(v)}{\eta_{st}} $\n",
        "    We threshold $C_B$ to extract the final 1-pixel wide skeleton.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_avignon"
      },
      "source": [
        "## Part 1: Case Study - Palais des Papes (Avignon)\n",
        "\n",
        "We demonstrate the power of fusion on a challenging real-world case: the retaining rock of the Palais des Papes. The intensity image suffers from deep shadows that \"erase\" the fault, while the photogrammetric depth map captures the geometry but is noisy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avignon_download"
      },
      "outputs": [],
      "source": [
        "# Download Avignon Data\n",
        "avignon_dir = \"data_avignon\"\n",
        "os.makedirs(avignon_dir, exist_ok=True)\n",
        "\n",
        "# File IDs from Google Drive\n",
        "ortho_id = \"1OXK3XNdrirwvnwI5yZI2AKlh3qSiSeDd\"\n",
        "mne_id = \"1iB6RllC9augWlAshqE2e08vnglHHM_PQ\"\n",
        "\n",
        "ortho_path = os.path.join(avignon_dir, \"Ortho_new_extrait.tif\")\n",
        "mne_path = os.path.join(avignon_dir, \"MNE_new_extrait.mat\")\n",
        "\n",
        "if not os.path.exists(ortho_path):\n",
        "    gdown.download(id=ortho_id, output=ortho_path, quiet=False)\n",
        "\n",
        "if not os.path.exists(mne_path):\n",
        "    gdown.download(id=mne_id, output=mne_path, quiet=False)\n",
        "\n",
        "print(\"Avignon data downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avignon_load"
      },
      "outputs": [],
      "source": [
        "# Helper function to load .mat files\n",
        "def load_mat_2d(path, key=None, dtype=np.float32, squeeze=True):\n",
        "    \"\"\"
",
        "    Loads a .mat file and returns a 2D np.ndarray in float32.\n",
        "    Handles both old scipy.io and new v7.3 HDF5 formats.\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "    try:\n",
        "        # Try scipy.io.loadmat (files <= v7.2)\n",
        "        from scipy.io import loadmat\n",
        "        mdict = loadmat(path)\n",
        "        user_keys = [k for k in mdict.keys() if not k.startswith('__')]\n",
        "        if key is None:\n",
        "            if len(user_keys) != 1:\n",
        "                raise ValueError(f\"Cannot guess variable: {user_keys}. Specify `key=`.\")\n",
        "            key = user_keys[0]\n",
        "        if key not in mdict:\n",
        "            raise ValueError(f\"Variable '{key}' not found in {path}.\")\n",
        "        arr = mdict[key]\n",
        "    except NotImplementedError:\n",
        "        # Fallback to HDF5 (v7.3)\n",
        "        import h5py\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            available = list(f.keys())\n",
        "            if key is None:\n",
        "                if len(available) != 1:\n",
        "                    raise ValueError(f\"Cannot guess variable: {available}. Specify `key=`.\")\n",
        "                key = available[0]\n",
        "            if key not in f:\n",
        "                raise ValueError(f\"Variable '{key}' not found in {path}.\")\n",
        "            arr = np.array(f[key])\n",
        "\n",
        "    if squeeze: arr = np.squeeze(arr)\n",
        "    if arr.ndim != 2:\n",
        "        raise ValueError(f\"Variable '{key}' is not 2D after squeeze (shape={arr.shape}).\")\n",
        "    return np.asarray(arr, dtype=dtype)\n",
        "\n",
        "# --- Load and Preprocess Avignon Images --- \n",
        "IMG_PATH = ortho_path\n",
        "assert os.path.exists(IMG_PATH), f\"File not found: {IMG_PATH}\"\n",
        "\n",
        "# Load Image\n",
        "img = io.imread(IMG_PATH)\n",
        "if img.ndim == 2:\n",
        "    img_gray = img_as_float32(img)\n",
        "    img_rgb = None\n",
        "elif img.ndim == 3 and img.shape[-1] == 3:\n",
        "    img_rgb = img\n",
        "    img_gray = img_as_float32(color.rgb2gray(img_rgb))\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported image format. Shape: {img.shape}\")\n",
        "\n",
        "print(f\"Image loaded: {img_gray.shape} pixels – Intensity ∈ [{img_gray.min():.3f}, {img_gray.max():.3f}]\")\n",
        "\n",
        "# Load Depth (MNE)\n",
        "Scale_Z = 100.0  # Z scale factor\n",
        "F = 2            # Downsampling factor\n",
        "\n",
        "depth_raw = load_mat_2d(mne_path, key=\"mne\")\n",
        "depth_map = Scale_Z / F * depth_raw\n",
        "\n",
        "print(\"MNE loaded:\", depth_map.shape, depth_map.dtype)\n",
        "\n",
        "if img_gray.shape != depth_map.shape:\n",
        "    # Handle slight shape mismatch if necessary, or raise error\n",
        "    # Here we resize to match img_gray if close, or strict check\n",
        "    if img_gray.shape != depth_map.shape:\n",
        "         # Simple resize of depth to match image if dimensions differ slightly\n",
        "         depth_map = resize(depth_map, img_gray.shape, order=1, preserve_range=True)\n",
        "\n",
        "# Resize both for performance\n",
        "new_shape = (img_gray.shape[0] // F, img_gray.shape[1] // F)\n",
        "img_gray_small = resize(img_gray, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "depth_map_small = resize(depth_map, new_shape, order=1, anti_aliasing=True, preserve_range=True)\n",
        "\n",
        "print(f\"Resized by factor {F}. New shape: {img_gray_small.shape}\")\n",
        "\n",
        "# Save previews\n",
        "iio.imwrite(os.path.join(avignon_dir, \"img_gray_small.png\"), ((img_gray_small/img_gray_small.max()) * 65535).astype(np.uint16))\n",
        "iio.imwrite(os.path.join(avignon_dir, \"depth_map_small.png\"), (((depth_map_small - depth_map_small.min())/(depth_map_small.max() - depth_map_small.min())) * 65535).astype(np.uint16))\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img_gray_small, cmap='gray')\n",
        "ax[0].set_title(\"Intensity (Gray)\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(img_gray_small, cmap='gray')\n",
        "im = ax[1].imshow(depth_map_small, cmap='seismic', alpha=0.6, vmin=depth_map_small.min(), vmax=depth_map_small.max())\n",
        "ax[1].set_title(\"Intensity + Depth Overlay\")\n",
        "ax[1].axis('off')\n",
        "plt.colorbar(im, ax=ax[1], fraction=0.046, pad=0.04, label=\"Depth\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avignon_processing"
      },
      "outputs": [],
      "source": [
        "# --- Run Extraction Algorithm on Avignon Data ---\n",
        "\n",
        "def run_pipeline(img_input, modality_name):\n",
        "    \"\"\"Runs the full extraction pipeline on a single modality image.\"\"\"\n",
        "    # 1. Compute Hessians\n",
        "    hessians = compute_hessians_per_scale(img_input, Σ)\n",
        "\n",
        "    # 2. \"Fusion\" (Single modality here acts as fusion of 1 input)\n",
        "    # We create a dictionary for the fusion function\n",
        "    mods = {modality_name: hessians}\n",
        "    weights = {modality_name: 1.0}\n",
        "    fused_H = fuse_hessians_per_scale(mods, weights)\n",
        "\n",
        "    # 3. Build Graph\n",
        "    coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, dark_ridges=dark_ridges)\n",
        "    D = distances_from_similarity(S, mode=\"minus\")\n",
        "    if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "\n",
        "    # 4. Connected Components\n",
        "    D_cc, idx_nodes = largest_connected_component(D)\n",
        "    if D_cc.shape[0] == 0:\n",
        "        return fused_H, np.zeros_like(img_input)\n",
        "\n",
        "    sub_coords = coords[idx_nodes]\n",
        "\n",
        "    # 5. HDBSCAN Clustering\n",
        "    Dist = D_cc.copy()\n",
        "    Dist.setdiag(0.0)\n",
        "    labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "\n",
        "    # 6. MST & Betweenness Centrality\n",
        "    all_edges = []\n",
        "    for lab in np.unique(labels):\n",
        "        if lab < 0: continue\n",
        "        cl = np.where(labels == lab)[0]\n",
        "        if cl.size < 3: continue\n",
        "\n",
        "        mst = mst_on_cluster(D_cc, cl)\n",
        "        global_indices = idx_nodes[cl]\n",
        "        S_cluster = S[global_indices, :][:, global_indices]\n",
        "\n",
        "        # Extract Backbone\n",
        "        nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "        segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "        if segs.shape[0] > 0:\n",
        "            all_edges.append(segs)\n",
        "\n",
        "    # Rasterize Result\n",
        "    mask = np.zeros_like(img_input)\n",
        "    if all_edges:\n",
        "        fault_edges = np.vstack(all_edges)\n",
        "        for e in fault_edges:\n",
        "            r0, c0, r1, c1, _ = e\n",
        "            rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "            rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "            mask[rr, cc] = 1.0\n",
        "\n",
        "    return fused_H, mask\n",
        "\n",
        "print(\"Processing Intensity Only...\")\n",
        "H_int, mask_int = run_pipeline(img_gray_small, \"intensity\")\n",
        "\n",
        "print(\"Processing Depth Only...\")\n",
        "# Depth often has range-specific noise; we assume standard 'dark_ridges' apply to depth valleys too\n",
        "H_depth, mask_depth = run_pipeline(depth_map_small, \"depth\")\n",
        "\n",
        "print(\"Processing Fusion (Intensity + Depth)...")\n",
        "# Manual Fusion Step\n",
        "mods_fusion = {\n",
        "    \"intensity\": compute_hessians_per_scale(img_gray_small, Σ),\n",
        "    \"depth\": compute_hessians_per_scale(depth_map_small, Σ)\n",
        "}\n",
        "weights_fusion = {\"intensity\": 0.66, \"depth\": 0.33} # 2/3 Intensity, 1/3 Depth as per paper\n",
        "fused_H_final = fuse_hessians_per_scale(mods_fusion, weights_fusion)\n",
        "\n",
        "# Run graph part on fused Hessian\n",
        "coords, _, S = build_frangi_similarity_graph(fused_H_final, β, c, c_θ, R, dark_ridges=dark_ridges)\n",
        "D = distances_from_similarity(S, mode=\"minus\")\n",
        "if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "mask_fusion = np.zeros_like(img_gray_small)\n",
        "if D_cc.shape[0] > 0:\n",
        "    Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "    labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size)\n",
        "    sub_coords = coords[idx_nodes]\n",
        "    all_edges = []\n",
        "    for lab in np.unique(labels):\n",
        "        if lab < 0: continue\n",
        "        cl = np.where(labels == lab)[0]\n",
        "        if cl.size < 3: continue\n",
        "        mst = mst_on_cluster(D_cc, cl)\n",
        "        global_indices = idx_nodes[cl]\n",
        "        S_cluster = S[global_indices, :][:, global_indices]\n",
        "        nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_threshold, S=S_cluster, take_similarity=True)\n",
        "        segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "        if segs.shape[0] > 0: all_edges.append(segs)\n",
        "    if all_edges:\n",
        "        fault_edges = np.vstack(all_edges)\n",
        "        for e in fault_edges:\n",
        "            r0, c0, r1, c1, _ = e\n",
        "            rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "            rr, cc = np.clip(rr.astype(int), 0, mask_fusion.shape[0]-1), np.clip(cc.astype(int), 0, mask_fusion.shape[1]-1)\n",
        "            mask_fusion[rr, cc] = 1.0\n",
        "\n",
        "print(\"Avignon processing complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avignon_results"
      },
      "outputs": [],
      "source": [
        "# --- Visualize Avignon Results ---\n",
        "def get_best_lambda2(hess_list):\n",
        "    # Stack |λ2| and take max across scales\n",
        "    e2n_stack = np.stack([Hd['e2n'] for Hd in hess_list], axis=0)\n",
        "    abs_e2n_stack = np.abs(e2n_stack)\n",
        "    best_idx = abs_e2n_stack.argmax(axis=0)\n",
        "    return np.take_along_axis(e2n_stack, best_idx[None,...], axis=0)[0]\n",
        "\n",
        "l2_int = get_best_lambda2(H_int)\n",
        "l2_depth = get_best_lambda2(H_depth)\n",
        "l2_fusion = get_best_lambda2(fused_H_final)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "# Row 1: Inputs\n",
        "axes[0,0].imshow(img_gray_small, cmap='gray'); axes[0,0].set_title(\"Input: Intensity\")\n",
        "axes[0,1].imshow(depth_map_small, cmap='inferno'); axes[0,1].set_title(\"Input: Depth (MNE)\")\n",
        "axes[0,2].imshow(img_gray_small, cmap='gray')\n",
        "axes[0,2].imshow(depth_map_small, cmap='seismic', alpha=0.5); axes[0,2].set_title(\"Overlay\")\n",
        "\n",
        "# Row 2: Frangi Response (|λ2|)\n",
        "axes[1,0].imshow(np.clip(l2_int, 0, 1), cmap='magma'); axes[1,0].set_title(\"Intensity |λ₂|\")\n",
        "axes[1,1].imshow(np.clip(l2_depth, 0, 1), cmap='magma'); axes[1,1].set_title(\"Depth |λ₂|\")\n",
        "axes[1,2].imshow(np.clip(l2_fusion, 0, 1), cmap='magma'); axes[1,2].set_title(\"Fusion |λ₂|\")\n",
        "\n",
        "# Row 3: Extracted Skeletons\n",
        "axes[2,0].imshow(mask_int, cmap='gray'); axes[2,0].set_title(\"Result: Intensity Only\")\n",
        "axes[2,1].imshow(mask_depth, cmap='gray'); axes[2,1].set_title(\"Result: Depth Only\")\n",
        "axes[2,2].imshow(mask_fusion, cmap='Reds'); axes[2,2].set_title(\"Result: Fusion\")\n",
        "\n",
        "for ax in axes.flat: ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_find"
      },
      "source": [
        "## Part 2: FIND Dataset Benchmark\n",
        "\n",
        "We now proceed to the quantitative evaluation on the FIND dataset (first 500 images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_download"
      },
      "outputs": [],
      "source": [
        "# Download FIND Dataset\n",
        "import zipfile\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1qnLMCeon7LJjT9H0ENiNF5sFs-F7-NvK\"\n",
        "zip_path = \"data.zip\"\n",
        "extract_dir = \"data_find\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "if not os.path.exists(zip_path):\n",
        "    gdown.download(url, zip_path, quiet=False)\n",
        "\n",
        "if not os.path.exists(os.path.join(extract_dir, \"00001.png\")):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Unzipped FIND dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_single"
      },
      "outputs": [],
      "source": [
        "# Single Image Example (Seed = 1)\n",
        "seed = 1\n",
        "struct = auto_discover_find_structure(\"data_find\")\n",
        "dat = load_modalities_and_gt_by_index(struct, seed)\n",
        "\n",
        "print(\"Processing Sample Image index:\", seed)\n",
        "# Show modalities\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (k, arr) in enumerate(dat[\"arrays\"].items()):\n",
        "    plt.subplot(1, len(dat[\"arrays\"].items()), i+1)\n",
        "    plt.title(k)\n",
        "    plt.imshow(arr, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_batch"
      },
      "outputs": [],
      "source": [
        "# --- Batch Processing Configuration ---\n",
        "\n",
        "excluded_ids = []  # List of IDs to exclude (e.g. [10, 25, 30])\n",
        "start_idx = 0\n",
        "end_idx = 500\n",
        "n_jobs = 8\n",
        "\n",
        "# Weights for FIND dataset (tuned for generic performance)\n",
        "weights = {\"intensity\": 0.4, \"range\": 0.1, \"filtered\": 0.5, \"fused\": 0.0}\n",
        "\n",
        "# Output setup\n",
        "output_dir = \"Results_FIND\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def process_image_idx(idx):\n",
        "    if idx in excluded_ids:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dat = load_modalities_and_gt_by_index(struct, idx)\n",
        "        base = dat[\"arrays\"].get(\"intensity\", next(iter(dat['arrays'].values())))\n",
        "        img_name = f\"im{idx+1:05d}\"\n",
        "\n",
        "        # Load Ground Truth\n",
        "        gt_raw = dat[\"arrays\"].get(\"label\", np.zeros_like(base))\n",
        "        gt = (gt_raw > 0).astype(np.uint8)\n",
        "        # Morphological Closing/Opening on GT to match thick cracks\n",
        "        gt = binary_closing(gt, footprint=disk(2)).astype(np.uint8)\n",
        "        gt = binary_opening(gt, footprint=disk(2)).astype(np.uint8)\n",
        "        sk_gt = skeletonize_lee(gt)\n",
        "        sk_gt_thick = thicken(sk_gt, pixels=3)\n",
        "\n",
        "        # Compute Hessians & Fuse\n",
        "        mods_hess = {}\n",
        "        valid_keys = [k for k in weights.keys() if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "        for k in valid_keys:\n",
        "            mods_hess[k] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][k]), Σ)\n",
        "\n",
        "        fused_H = fuse_hessians_per_scale(mods_hess, weights)\n",
        "\n",
        "        # Graph & Extraction\n",
        "        coords, _, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, dark_ridges=dark_ridges)\n",
        "        D = distances_from_similarity(S, mode=\"minus\")\n",
        "        if K == 2: D = triangle_connectivity_graph(coords, D)\n",
        "        D_cc, idx_nodes = largest_connected_component(D)\n",
        "\n",
        "        sk_pred = np.zeros_like(base, dtype=np.uint8)\n",
        "        if D_cc.shape[0] > 0:\n",
        "            Dist = D_cc.copy(); Dist.setdiag(0.0)\n",
        "            # HDBSCAN\n",
        "            labels = hdbscan_from_sparse(Dist, min_cluster_size=min_cluster_size, allow_single_cluster=True)\n",
        "            sub_coords = coords[idx_nodes]\n",
        "            all_fault_edges = []\n",
        "            for lab in np.unique(labels):\n",
        "                if lab < 0: continue\n",
        "                cl = np.where(labels == lab)[0]\n",
        "                if cl.size < 3: continue\n",
        "                mst = mst_on_cluster(D_cc, cl)\n",
        "                global_indices = idx_nodes[cl]\n",
        "                S_cluster = S[global_indices, :][:, global_indices]\n",
        "                nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=0.2, S=S_cluster, take_similarity=True)\n",
        "                segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "                if segs.shape[0] > 0: all_fault_edges.append(segs)\n",
        "\n",
        "            if all_fault_edges:\n",
        "                fault_edges = np.vstack(all_fault_edges)\n",
        "                mask = np.zeros_like(base, dtype=np.uint8)\n",
        "                for e in fault_edges:\n",
        "                    r0, c0, r1, c1, _ = e\n",
        "                    rr, cc = np.linspace(r0, r1, int(max(abs(r1-r0), abs(c1-c0))+1)), np.linspace(c0, c1, int(max(abs(r1-r0), abs(c1-c0))+1))\n",
        "                    rr, cc = np.clip(rr.astype(int), 0, mask.shape[0]-1), np.clip(cc.astype(int), 0, mask.shape[1]-1)\n",
        "                    mask[rr, cc] = 1\n",
        "                sk_pred = skeletonize_lee(mask)\n",
        "\n",
        "        # Metrics\n",
        "        sk_pred_thick = thicken(sk_pred, pixels=3)\n",
        "        jac = jaccard_index(sk_pred_thick, sk_gt_thick)\n",
        "        tvs = tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5)\n",
        "        wass = wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)\n",
        "\n",
        "        return {\n",
        "            \"Image\": img_name,\n",
        "            \"Jaccard\": jac,\n",
        "            \"Tversky\": tvs,\n",
        "            \"Wasserstein\": wass\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(f\"Starting batch processing for images {start_idx+1}-{end_idx}...\")\n",
        "with tqdm_joblib(tqdm(total=end_idx-start_idx)) as progress_bar:\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_image_idx)(i) for i in range(start_idx, end_idx)\n",
        "    )\n",
        "\n",
        "# Filter None (excluded or errors)\n",
        "results = [r for r in results if r is not None]\n",
        "df_res = pd.DataFrame(results)\n",
        "\n",
        "# Summary\n",
        "if not df_res.empty:\n",
        "    mean_metrics = df_res[[\"Jaccard\", \"Tversky\", \"Wasserstein\"]].mean()\n",
        "    print(\"\\n--- Batch Results (Valid Images) ---\")\n",
        "    print(mean_metrics)\n",
        "    df_res.to_csv(os.path.join(output_dir, \"metrics.csv\"), index=False)\n",
        "else:\n",
        "    print(\"No valid results.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}