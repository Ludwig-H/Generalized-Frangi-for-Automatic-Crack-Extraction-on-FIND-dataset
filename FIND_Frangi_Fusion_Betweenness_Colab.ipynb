{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKBqM5O_ZvWZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "id": "uKBqM5O_ZvWZ"
    },
    {
      "cell_type": "markdown",
      "id": "d4ecf6ba",
      "metadata": {
        "id": "d4ecf6ba"
      },
      "source": [
        "\n",
        "# Generalized Frangi with Multi-modal Fusion on FIND (Betweenness Centrality Backbone)\n",
        "\n",
        "This notebook illustrates all steps with formulas and figures.\n",
        "- $\\Sigma$: set of scales, σ: a single scale\n",
        "- $\\lambda_1, \\lambda_2$: Hessian eigenvalues sorted so that $|\\lambda_1| \\le |\\lambda_2$- $\\beta$, $c$, $c_\\theta$: Frangi hyper-parameters\n",
        "- $R$: pixel neighborhood radius; $K \\in \\{1,2\\}$ controls triangle-connectivity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53173dcf",
      "metadata": {
        "id": "53173dcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install runtime deps (Colab-friendly)\n",
        "!pip -q install numpy scipy scikit-image matplotlib joblib tqdm tqdm-joblib hdbscan networkx gdown tifffile imageio pandas Pillow pot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/Ludwig-H/Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset.git\n",
        "cd Generalized-Frangi-for-Automatic-Crack-Extraction-on-FIND-dataset\n",
        "pip install  .\n",
        "cd .."
      ],
      "metadata": {
        "id": "LXiU9DdjiYG6"
      },
      "id": "LXiU9DdjiYG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9b9b9b",
      "metadata": {
        "id": "da9b9b9b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, sys, numpy as np, matplotlib, random, zipfile, imageio, pandas as pd\n",
        "# matplotlib.use('Agg') # Force non-interactive backend for safety in parallel loops\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from joblib import Parallel, delayed\n",
        "from skimage.morphology import binary_closing, binary_opening, disk\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure local src is importable in Colab\n",
        "repo_root = os.path.abspath(\"..\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.append(repo_root)\n",
        "\n",
        "from frangi_fusion import (\n",
        "    set_seed, auto_discover_find_structure, load_modalities_and_gt_by_index, to_gray_uint8,\n",
        "    to_gray, compute_hessians_per_scale, fuse_hessians_per_scale,\n",
        "    build_frangi_similarity_graph, distances_from_similarity, triangle_connectivity_graph,\n",
        "    largest_connected_component, hdbscan_from_sparse,\n",
        "    mst_on_cluster, kcenters_on_tree, paths_between_centers, skeleton_from_center_paths,\n",
        "    overlay_hessian_orientation, show_clusters_on_image, animate_fault_growth,\n",
        "    skeletonize_lee, thicken, jaccard_index, tversky_index, wasserstein_distance_skeletons,\n",
        "    extract_backbone_centrality, skeleton_from_mst_graph\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634cda40",
      "metadata": {
        "id": "634cda40"
      },
      "source": [
        "\n",
        "## 1) Download FIND via `gdown` then unzip\n",
        "\n",
        "We fetch `data.zip` and unpack to `data_find/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb5f921",
      "metadata": {
        "id": "6cb5f921"
      },
      "outputs": [],
      "source": [
        "import gdown, zipfile, os\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1qnLMCeon7LJjT9H0ENiNF5sFs-F7-NvK\"\n",
        "zip_path = \"data.zip\"\n",
        "extract_dir = \"data_find\"\n",
        "fallback_folder_url = \"https://drive.google.com/drive/folders/19lZyRzfMDcJpTMWxfgMNswgf8X4J3kZ5?usp=sharing\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Tentative 1: téléchargement du ZIP puis unzip\n",
        "    if not os.path.exists(zip_path):\n",
        "        out = gdown.download(url, zip_path, quiet=False)\n",
        "        if out is None or not os.path.exists(zip_path):\n",
        "            raise RuntimeError(\"gdown did not retrieve the ZIP.\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Unzipped to:\", extract_dir)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Primary download/unzip failed:\", repr(e))\n",
        "    print(\"Falling back to the public Drive folder (already unzipped)...\")\n",
        "\n",
        "    # Tentative 2: téléchargement récursif du dossier déjà dézippé\n",
        "    # Remplit directement extract_dir\n",
        "    try:\n",
        "        # gdown >= 4.2.0\n",
        "        gdown.download_folder(\n",
        "            url=fallback_folder_url,\n",
        "            output=extract_dir,\n",
        "            quiet=False,\n",
        "            use_cookies=False\n",
        "        )\n",
        "        print(\"Fallback synced to:\", extract_dir)\n",
        "    except Exception as e2:\n",
        "        print(\"Fallback folder download failed:\", repr(e2))\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a046ed8c",
      "metadata": {
        "id": "a046ed8c"
      },
      "source": [
        "\n",
        "## 2) Pick one image and show modalities\n",
        "\n",
        "We select one index with a fixed seed ($=1234$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080fcebe",
      "metadata": {
        "id": "080fcebe"
      },
      "outputs": [],
      "source": [
        "\n",
        "seed = 1 # set_seed(1234)\n",
        "struct = auto_discover_find_structure(\"data_find\")\n",
        "n_total = len(struct[\"label\"]) if struct[\"label\"] else len(struct[\"intensity\"])\n",
        "index = seed # random.Random(seed).randrange(max(1, n_total)) # or : index = ...\n",
        "dat = load_modalities_and_gt_by_index(struct, index)\n",
        "\n",
        "# Display modalities\n",
        "cols = len(dat[\"arrays\"])\n",
        "plt.figure(figsize=(4*cols,4))\n",
        "for i,(k,arr) in enumerate(dat[\"arrays\"].items()):\n",
        "    plt.subplot(1, cols, i+1); plt.title(k); plt.imshow(arr, cmap='gray'); plt.axis('off')\n",
        "plt.show()\n",
        "base = dat[\"arrays\"].get(\"intensity\", next(iter(dat['arrays'].values())))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# struct"
      ],
      "metadata": {
        "id": "hA908oAMI2vJ"
      },
      "id": "hA908oAMI2vJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5b0a5189",
      "metadata": {
        "id": "5b0a5189"
      },
      "source": [
        "\n",
        "## 3) Parameters\n",
        "\n",
        "- $\\Sigma = \\left\\{ \\sigma_m\\right\\}_{m=1}^M$: set of Gaussian scales  \n",
        "- $\\beta$, $c$, $c_\\theta$: Frangi parameters  \n",
        "- $R$: radius for pixel graph  \n",
        "- $K \\in \\{1,2\\}$: 1 = plain Frangi distances; 2 = add triangle-connectivity (Rips)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33479c56",
      "metadata": {
        "id": "33479c56"
      },
      "outputs": [],
      "source": [
        "Σ = [1,3,5,7,9] # 1,3,5,9\n",
        "β = 0.5\n",
        "c = 0.25\n",
        "c_θ = 0.125\n",
        "R = 5\n",
        "K = 1\n",
        "dark_ridges = True  # cracks are valleys => λ2 ≥ 0 after our sign convention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ffba61",
      "metadata": {
        "id": "90ffba61"
      },
      "source": [
        "\n",
        "## 4) Hessian per modality and per scale\n",
        "\n",
        "For a grayscale image $I$, the Hessian at scale σ is:\n",
        "$$\n",
        "H_\\sigma(I) =\n",
        "\\begin{pmatrix}\n",
        "I_{xx} & I_{xy} \\\\nI_{xy} & I_{yy}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "with Gaussian derivatives and reflective borders.\n",
        "\n",
        "We sort eigenvalues by absolute value at each pixel: $|\\lambda_1| \\le |\\lambda_2|$.  \n",
        "**Normalization:** for each σ we divide by $\\max_{x,y} |\\lambda_2(x,y)|$ so that $\\lambda_1,\\lambda_2 \\in [-1,1]$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3adf2a20",
      "metadata": {
        "id": "3adf2a20"
      },
      "outputs": [],
      "source": [
        "\n",
        "mods = {}\n",
        "if \"intensity\" in dat[\"arrays\"]:\n",
        "    mods[\"intensity\"] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][\"intensity\"]), Σ)\n",
        "if \"range\" in dat[\"arrays\"]:\n",
        "    mods[\"range\"] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][\"range\"]), Σ)\n",
        "if \"fused\" in dat[\"arrays\"]:\n",
        "    mods[\"fused\"] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][\"fused\"]), Σ)\n",
        "if \"filtered\" in dat[\"arrays\"]:\n",
        "    mods[\"filtered\"] = compute_hessians_per_scale(to_gray(dat[\"arrays\"][\"filtered\"]), Σ)\n",
        "\n",
        "weights = {\"intensity\": 0.4, \"range\":0.1, \"filtered\":0.5, \"fused\":0.} # {\"intensity\": 0.1, \"range\":0.8, \"filtered\":0.1, \"fused\":0.} #{k:1.0 for k in mods.keys()}\n",
        "fused_H = fuse_hessians_per_scale(mods, weights)\n",
        "print(\"Modalities fused:\", list(mods.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250f66c0",
      "metadata": {
        "id": "250f66c0"
      },
      "source": [
        "### Visualize: orientation-colored overlays and $|\\lambda_2|$\n",
        "We show $\\theta$ (color hue) and strength (value). For $|\\lambda_2|$ we select, per pixel, the $\\sigma$ that maximizes $|\\lambda_2|$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b87740",
      "metadata": {
        "id": "66b87740"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Single legend at left + 4 overlays\n",
        "import matplotlib as mpl\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "gs = fig.add_gridspec(nrows=1, ncols=5, width_ratios=[0.06,1,1,1,1], wspace=0.05)\n",
        "cax = fig.add_subplot(gs[0,0])\n",
        "sm = mpl.cm.ScalarMappable(cmap=mpl.cm.hsv, norm=mpl.colors.Normalize(vmin=-90, vmax=90))\n",
        "cb = fig.colorbar(sm, cax=cax); cb.set_label(\"θ (deg)\", fontsize=9); cb.set_ticks([-90,-45,0,45,90])\n",
        "for i,Hd in enumerate(fused_H[:4]):\n",
        "    ax = fig.add_subplot(gs[0,i+1])\n",
        "    overlay = overlay_hessian_orientation(base, Hd, alpha=0.5)\n",
        "    ax.imshow(overlay); ax.set_title(f\"σ={Hd['sigma']}\"); ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# |λ2| at best scale\n",
        "e2n_stack = np.stack([Hd['e2n'] for Hd in fused_H], axis=0)\n",
        "abs_e2n_stack = np.abs(e2n_stack)\n",
        "best_idx = abs_e2n_stack.argmax(axis=0)\n",
        "e2n_best = np.take_along_axis(e2n_stack, best_idx[None,...], axis=0)[0]\n",
        "plt.figure(figsize=(5,5)); plt.imshow(base, cmap='gray'); im=plt.imshow(np.clip(e2n_best,0,1), cmap='magma', alpha=0.65, vmin=0, vmax=1)\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04, label=\"|λ₂| normalized\"); plt.axis('off'); plt.title(\"|λ₂| (best σ)\"); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(fused_H[0]['e2n'])\n",
        "plt.colorbar()\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "tyXC5iuKxcSr"
      },
      "id": "tyXC5iuKxcSr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6f0bc9fb",
      "metadata": {
        "id": "6f0bc9fb"
      },
      "source": [
        "\n",
        "## 5) Frangi similarity graph $S$\n",
        "\n",
        "For neighbor pixels $i,j$ within radius $R$, we define a similarity (Frangi-style):\n",
        "$$\n",
        "s(i,j)= \\exp\\!\\left(-\\tfrac12\\left(\\tfrac{\\left|{\\frac{\\lambda_{1,i}}{\\lambda_{2,i}}}\\right| + \\left|{\\frac{\\lambda_{1,j}}{\\lambda_{2,j}}}\\right|}{\\beta}\\right)^2\\right)\n",
        "\\left(1-\\exp\\!\\left(-\\tfrac12\\left(\\tfrac{\\lvert \\lambda_{2,i}\\lambda_{2,j}\\rvert}{c}\\right)^2\\right)\\right)\n",
        "\\exp\\!\\left(-\\tfrac12\\left(\\tfrac{\\lvert \\sin(\\theta_i-\\theta_j)\\rvert}{c_\\theta}\\right)^2\\right)\n",
        "$$\n",
        "\n",
        "~~We keep only valleys if `dark_ridges=True` (λ₂ ≥ 0).~~\n",
        "We keep only large values: `v >= percentile(Frangi_response, threshold_mask)`.\n",
        "Distances are\n",
        "$$\n",
        "\\require{cancel}\n",
        "{\\texttt{mode=\"minus\"}} {D = 1 - S}.$$$$ {\\texttt{mode=\"inverse\"}} \\xcancel{D = 1/S -1} $$$$ {\\texttt{mode=\"log\"}} \\xcancel{D = \\sqrt{-\\log(S)}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736d952d",
      "metadata": {
        "id": "736d952d"
      },
      "outputs": [],
      "source": [
        "\n",
        "mode = \"minus\" # [\"log\", \"minus\", \"inverse\"]\n",
        "threshold_mask = 0.75 # 0.80 pour K=1\n",
        "coords, neighbors, S = build_frangi_similarity_graph(fused_H, β, c, c_θ, R, candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges) # threshold_mask = quantile we throw\n",
        "D = distances_from_similarity(S, mode)\n",
        "print(\"Graph:\", D.shape, \"nnz:\", D.nnz)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9281383e",
      "metadata": {
        "id": "9281383e"
      },
      "source": [
        "\n",
        "## 6) Triangle-connectivity ($K=2$, optionnal)\n",
        "\n",
        "We connect edges via Vietoris–Rips triangles (filtration = max of edge distances).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a924f14c",
      "metadata": {
        "id": "a924f14c"
      },
      "outputs": [],
      "source": [
        "\n",
        "if K==2:\n",
        "    D = triangle_connectivity_graph(coords, D)\n",
        "    print(\"After triangle‑connectivity:\", D.shape, \"nnz:\", D.nnz)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabefb8d",
      "metadata": {
        "id": "fabefb8d"
      },
      "source": [
        "\n",
        "## 7) Largest connected component\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46bbaca",
      "metadata": {
        "id": "c46bbaca"
      },
      "outputs": [],
      "source": [
        "\n",
        "D_cc, idx_nodes = largest_connected_component(D)\n",
        "sub_coords = coords[idx_nodes]\n",
        "print(\"Largest CC:\", D_cc.shape[0], \"nodes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0428ad68",
      "metadata": {
        "id": "0428ad68"
      },
      "source": [
        "\n",
        "## 8) Sparse HDBSCAN on CSR distances (no densification)\n",
        "\n",
        "We compute **core distances** from sparse neighbors, **mutual reachability**, the **MST**, and apply a simplified **EOM** selection.\n",
        "`expZ` is the exponent used for preprocessing the radius $r$ in the distance matrix (used to compute the *stability* $\\hat \\lambda$ in `excess of mass`EOM method):\n",
        "$$\n",
        "r \\mapsto r^{\\texttt{expZ}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753c72da",
      "metadata": {
        "id": "753c72da"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "\n",
        "expZ = 1\n",
        "\n",
        "Dist = D_cc.copy()\n",
        "Dist.data = Dist.data ** expZ\n",
        "Dist = Dist.tocsr()\n",
        "Dist.setdiag(0.0)\n",
        "min_cluster_size = 512\n",
        "max_dist = 1\n",
        "min_samples = 1\n",
        "allow_single_cluster = True\n",
        "# labels = hdbscan_from_sparse(Dist, min_cluster_size=200, min_samples=5, allow_single_cluster=True, expZ=expZ)\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    metric=\"precomputed\",\n",
        "    min_cluster_size=min_cluster_size,\n",
        "    min_samples=min_samples,\n",
        "    max_dist=max_dist,\n",
        "    allow_single_cluster=allow_single_cluster,\n",
        ")\n",
        "labels = clusterer.fit_predict(Dist)\n",
        "print(\"Clusters:\", np.unique(labels), \". 'Noise':\", np.sum(labels == -1))\n",
        "show_clusters_on_image(base, sub_coords, labels, figsize=(5,5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08afd0d",
      "metadata": {
        "id": "b08afd0d"
      },
      "source": [
        "\n",
        "## 9) MST + Betweenness Centrality Backbone → skeleton\n",
        "\n",
        "Replaces K-centers by a centrality drop criterion on the MST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8660b15",
      "metadata": {
        "id": "f8660b15"
      },
      "outputs": [],
      "source": [
        "\n",
        "from frangi_fusion.mst_kcenters import mst_on_cluster, extract_backbone_centrality, skeleton_from_mst_graph\n",
        "all_fault_edges = []\n",
        "uniq = np.unique(labels)\n",
        "f_seuil = 0.25\n",
        "\n",
        "for lab in uniq:\n",
        "    if lab < 0: continue\n",
        "    cl = np.where(labels==lab)[0]\n",
        "    if cl.size < 3: continue\n",
        "    mst = mst_on_cluster(D_cc, cl)\n",
        "\n",
        "    global_indices = idx_nodes[cl]\n",
        "    S_cluster = S[global_indices, :][:, global_indices]\n",
        "\n",
        "    # Betweenness Centrality Filtering\n",
        "    nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_seuil, S=S_cluster, take_similarity=True)\n",
        "    segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "\n",
        "    if segs.shape[0]>0: all_fault_edges.append(segs)\n",
        "\n",
        "fault_edges = np.vstack(all_fault_edges) if all_fault_edges else np.zeros((0,5), dtype=np.float32)\n",
        "print(\"Skeleton segments (Betweenness):\", fault_edges.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ee1621",
      "metadata": {
        "id": "f1ee1621"
      },
      "source": [
        "### Visualize skeleton on the base image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97822602",
      "metadata": {
        "id": "97822602"
      },
      "outputs": [],
      "source": [
        "τ = 1.0\n",
        "\n",
        "H,W = base.shape[:2]\n",
        "overlay = np.dstack([base,base,base]).astype(np.float32)\n",
        "for e in fault_edges[fault_edges[:,-1] <= τ]: # fault_edges:\n",
        "    r0,c0,r1,c1,w = e\n",
        "    rr = np.linspace(r0, r1, num=int(max(abs(r1-r0),abs(c1-c0))+1)).astype(int)\n",
        "    cc = np.linspace(c0, c1, num=rr.shape[0]).astype(int)\n",
        "    rr = np.clip(rr, 0, H-1); cc = np.clip(cc, 0, W-1)\n",
        "    overlay[rr,cc,0]=255; overlay[rr,cc,1]=0; overlay[rr,cc,2]=0\n",
        "plt.figure(figsize=(5,5)); plt.imshow(overlay.astype(np.uint8)); plt.axis('off'); plt.title(\"Skeleton\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c924271",
      "metadata": {
        "id": "5c924271"
      },
      "source": [
        "\n",
        "## 10) Filtration per pixel\n",
        "\n",
        "For each pixel \\(i\\) in the CC, the filtration value is $\\min_j d(i,j)$ over its neighbors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e1f8a8",
      "metadata": {
        "id": "90e1f8a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "n = D_cc.shape[0]\n",
        "indptr, indices, data = D_cc.indptr, D_cc.indices, D_cc.data\n",
        "filtration = np.full(n, np.inf, dtype=np.float64)\n",
        "for i in range(n):\n",
        "    row = data[indptr[i]:indptr[i+1]]\n",
        "    if row.size>0: filtration[i] = float(row.min())\n",
        "fmap = np.full(base.shape[:2], np.nan, dtype=np.float32)\n",
        "rr,cc = sub_coords[:,0], sub_coords[:,1]\n",
        "fmap[rr,cc] = filtration.astype(np.float32)\n",
        "vmin = np.nanpercentile(fmap,5); vmax = np.nanpercentile(fmap,95)\n",
        "plt.figure(figsize=(6,6)); plt.imshow(base, cmap='gray')\n",
        "im = plt.imshow(np.ma.masked_invalid(fmap), cmap='inferno', alpha=0.65, vmin=vmin, vmax=vmax)\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Filtration min d(i,·)\")\n",
        "plt.axis('off'); plt.title(\"Filtration overlay\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd31b785",
      "metadata": {
        "id": "cd31b785"
      },
      "source": [
        "\n",
        "## 11) Thresholded graph (τ = 0.8) and metrics vs GT\n",
        "\n",
        "We thicken fragments to compare skeletons more robustly. Metrics: **Jaccard**, **Tversky** (α=1, β=0.5), **Wasserstein**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6332f92a",
      "metadata": {
        "id": "6332f92a"
      },
      "outputs": [],
      "source": [
        "\n",
        "épaisseur_trait = 3 # pixels\n",
        "diam_affin = 3\n",
        "mask = np.zeros_like(base, dtype=np.uint8)\n",
        "for e in fault_edges[fault_edges[:,-1] <= τ]:\n",
        "    r0,c0,r1,c1,w = e\n",
        "    rr = np.linspace(r0, r1, num=int(max(abs(r1-r0),abs(c1-c0))+1)).astype(int)\n",
        "    cc = np.linspace(c0, c1, num=rr.shape[0]).astype(int)\n",
        "    rr = np.clip(rr, 0, base.shape[0]-1); cc = np.clip(cc, 0, base.shape[1]-1)\n",
        "    mask[rr,cc] = 1\n",
        "\n",
        "sk_pred = skeletonize_lee(mask); sk_pred = thicken(sk_pred, pixels=épaisseur_trait)\n",
        "gt = (dat[\"arrays\"].get(\"label\", np.zeros_like(base)) > 0).astype(np.uint8)\n",
        "\n",
        "if diam_affin > 0:\n",
        "    gt = binary_closing(gt, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "    gt = binary_opening(gt, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "\n",
        "sk_gt = skeletonize_lee(gt); sk_gt = thicken(sk_gt, pixels=épaisseur_trait)\n",
        "\n",
        "jac = jaccard_index(sk_pred, sk_gt)\n",
        "tvs = tversky_index(sk_pred, sk_gt, alpha=1.0, beta=0.5)\n",
        "wass = wasserstein_distance_skeletons(sk_pred, sk_gt)\n",
        "print(\"My Algo -> Jaccard:\", jac, \"Tversky:\", tvs, \"Wasserstein:\", wass)\n",
        "\n",
        "# --- CrackSegDiff Comparison (Single Image) --- ---\n",
        "csd_base_dir = \"/content/drive/MyDrive/Datasets/FIND/Results/CrackSegDiff/20000_1000/test_output_fused\"\n",
        "csd_path = os.path.join(csd_base_dir, f\"im{index+1:05d}_output_ens.png\")\n",
        "sk_csd_thick = None\n",
        "\n",
        "if os.path.exists(csd_path):\n",
        "    try:\n",
        "        csd_img = np.array(Image.open(csd_path).convert('L'))\n",
        "        csd_bin = (csd_img > 127).astype(np.uint8)\n",
        "        if diam_affin > 0:\n",
        "            csd_bin = binary_closing(csd_bin, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "            csd_bin = binary_opening(csd_bin, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "        sk_csd = skeletonize_lee(csd_bin)\n",
        "        sk_csd_thick = thicken(sk_csd, pixels=épaisseur_trait)\n",
        "\n",
        "        jac_csd = jaccard_index(sk_csd_thick, sk_gt)\n",
        "        tvs_csd = tversky_index(sk_csd_thick, sk_gt, alpha=1.0, beta=0.5)\n",
        "        wass_csd = wasserstein_distance_skeletons(sk_csd_thick, sk_gt)\n",
        "        print(\"CrackSegDiff -> Jaccard:\", jac_csd, \"Tversky:\", tvs_csd, \"Wasserstein:\", wass_csd)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing CSD for single image: {e}\")\n",
        "else :\n",
        "    print(f\"CSD path {csd_path} does not exist...\")\n",
        "\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.subplot(1,4,1); plt.title(\"GT thick skeleton\"); plt.imshow(sk_gt, cmap='gray'); plt.axis('off')\n",
        "plt.subplot(1,4,2); plt.title(\"CSD thick skeleton\"); plt.imshow(sk_csd_thick, cmap='gray'); plt.axis('off')\n",
        "plt.subplot(1,4,3); plt.title(\"Pred thick skeleton\"); plt.imshow(sk_pred, cmap='gray'); plt.axis('off')\n",
        "plt.subplot(1,4,4); plt.title(\"Overlay (R=GT, B=Ours, G=CSD)\");\n",
        "plt.imshow(sk_gt*255, cmap='Reds', alpha=0.7)\n",
        "plt.imshow(sk_pred*255, cmap='Blues', alpha=0.5)\n",
        "if sk_csd_thick is not None:\n",
        "    plt.imshow(sk_csd_thick*255, cmap='Greens', alpha=0.4)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbARx5uNZvWZ"
      },
      "source": [
        "# Batch Processing on Drive (Images 1-500)\n"
      ],
      "id": "rbARx5uNZvWZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59SBwqImZvWZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import hdbscan\n",
        "import itertools\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "\n",
        "# --- Configuration ---# Process images 1 to 500 (indices 0 to 499)\n",
        "start_idx = 0\n",
        "end_idx = 500\n",
        "n_jobs = 8 # Parallel CPUs\n",
        "\n",
        "# # Parameters (same as example)\n",
        "# Σ = [1,3,5,9]\n",
        "# β = 0.5\n",
        "# c = 0.25\n",
        "# c_θ = 0.125\n",
        "# R = 5\n",
        "# K = 1\n",
        "# dark_ridges = True\n",
        "# mode = \"minus\"\n",
        "# threshold_mask = 0.80\n",
        "# expZ = 1\n",
        "# min_cluster_size = 512\n",
        "# min_samples = 1\n",
        "# max_dist = 1\n",
        "# allow_single_cluster = True\n",
        "# τ = 0.9\n",
        "# weights = {\"intensity\": 0.7, \"range\":0.1, \"filtered\":0.2, \"fused\":0.}\n",
        "# épaisseur_trait = 3\n",
        "USE_COMBO = False # Set to True to enable Exhaustive Polarity (Norm/Inv combinations)\n",
        "# diam_affin = 2\n",
        "\n",
        "# --- Output Directory Setup ---# Construct folder name with parameters\n",
        "weights_str = f\"I{weights.get('intensity',0)}_R{weights.get('range',0)}_F{weights.get('filtered',0)}_Fu{weights.get('fused',0)}\"\n",
        "params_str = f\"K{K}_beta{β}_c{c}_ct{c_θ}_R{R}_Sigma{'_'.join(map(str, Σ))}_tau{τ}\"\n",
        "if USE_COMBO:\n",
        "    params_str += \"_ExhaustivePolarity\"\n",
        "else:\n",
        "    params_str += \"_SinglePolarity\"\n",
        "folder_name = f\"{weights_str}_{params_str}_Betweenness\"\n",
        "\n",
        "# Base path on Drive\n",
        "base_output_dir = \"/content/drive/MyDrive/Datasets/FIND/Results\"\n",
        "output_dir = os.path.join(base_output_dir, folder_name)\n",
        "skeletons_dir = os.path.join(output_dir, \"skeletons\")\n",
        "overlay_dir = os.path.join(output_dir, \"overlays\")\n",
        "\n",
        "print(f\"Results will be saved to: {output_dir}\")\n",
        "if not os.path.exists(skeletons_dir):\n",
        "    os.makedirs(skeletons_dir, exist_ok=True)\n",
        "if not os.path.exists(overlay_dir):\n",
        "    os.makedirs(overlay_dir, exist_ok=True)\n",
        "\n",
        "# --- Worker Function ---\n",
        "def process_image_idx(idx):\n",
        "    try:\n",
        "        # Ensure necessary imports inside worker if needed (though cloudpickle handles most)\n",
        "        # Note: 'struct' must be available in the scope or passed.\n",
        "        # In Colab/Jupyter + joblib, global vars from the notebook usually pickle fine.\n",
        "\n",
        "        # 1. Load Data\n",
        "        dat = load_modalities_and_gt_by_index(struct, idx)\n",
        "        base = dat[\"arrays\"].get(\"intensity\", next(iter(dat['arrays'].values())))\n",
        "        img_name = f\"im{idx+1:05d}\"\n",
        "\n",
        "        # Prepare GT\n",
        "        gt_raw = dat[\"arrays\"].get(\"label\", np.zeros_like(base))\n",
        "        gt = (gt_raw > 0).astype(np.uint8)\n",
        "\n",
        "        if 'diam_affin' not in globals(): diam_affin = 2\n",
        "        if diam_affin > 0:\n",
        "            gt = binary_closing(gt, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "            gt = binary_opening(gt, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "\n",
        "        sk_gt = skeletonize_lee(gt)\n",
        "        sk_gt_thick = thicken(sk_gt, pixels=épaisseur_trait)\n",
        "\n",
        "        # --- CrackSegDiff Comparison ---        # csd_base_dir = \"/content/drive/MyDrive/Datasets/FIND/Results/CrackSegDiff/200_5/test_output_fused\"\n",
        "        csd_path = os.path.join(csd_base_dir, f\"im{idx+1:05d}_output_ens.png\")\n",
        "        csd_metrics = {\"CSD_Jaccard\": np.nan, \"CSD_Tversky\": np.nan, \"CSD_Wasserstein\": np.nan}\n",
        "        sk_csd_thick = None\n",
        "\n",
        "        if os.path.exists(csd_path):\n",
        "            try:\n",
        "                csd_img = np.array(Image.open(csd_path).convert('L'))\n",
        "                csd_bin = (csd_img > 127).astype(np.uint8)\n",
        "                if diam_affin > 0:\n",
        "                    csd_bin = binary_closing(csd_bin, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "                    csd_bin = binary_opening(csd_bin, footprint=disk(diam_affin)).astype(np.uint8)\n",
        "                sk_csd = skeletonize_lee(csd_bin)\n",
        "                sk_csd_thick = thicken(sk_csd, pixels=épaisseur_trait)\n",
        "\n",
        "                csd_metrics[\"CSD_Jaccard\"] = jaccard_index(sk_csd_thick, sk_gt_thick)\n",
        "                csd_metrics[\"CSD_Tversky\"] = tversky_index(sk_csd_thick, sk_gt_thick, alpha=1.0, beta=0.5)\n",
        "                csd_metrics[\"CSD_Wasserstein\"] = wasserstein_distance_skeletons(sk_csd_thick, sk_gt_thick)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing CSD for {img_name}: {e}\")\n",
        "\n",
        "        # 2. Pre-compute Hessians\n",
        "        valid_mods = [k for k in [\"intensity\", \"range\", \"filtered\", \"fused\"] if k in dat[\"arrays\"] and weights[k] > 0]\n",
        "        hessian_cache = {}\n",
        "\n",
        "        for mod in valid_mods:\n",
        "            arr = dat[\"arrays\"][mod]\n",
        "            h_norm = compute_hessians_per_scale(to_gray(arr), Σ)\n",
        "            hessian_cache[mod] = [h_norm]\n",
        "            if USE_COMBO:\n",
        "                arr_inv = 255 - arr\n",
        "                h_inv = compute_hessians_per_scale(to_gray(arr_inv), Σ)\n",
        "                hessian_cache[mod].append(h_inv)\n",
        "\n",
        "        # 3. Exhaustive combinations or Single\n",
        "        if USE_COMBO:\n",
        "            combo_indices = list(itertools.product([0, 1], repeat=len(valid_mods)))\n",
        "        else:\n",
        "            # Just one combination: all zeros (normal modality)\n",
        "            combo_indices = [tuple(0 for _ in range(len(valid_mods)))]\n",
        "\n",
        "\n",
        "\n",
        "        best_tversky = -1.0\n",
        "        best_metrics = {\n",
        "            \"Image\": img_name,\n",
        "            \"Jaccard\": 0.0,\n",
        "            \"Tversky\": 0.0,\n",
        "            \"Wasserstein\": 0.0,\n",
        "            \"Best_Combo\": \"\"\n",
        "        }\n",
        "        best_sk_img = None\n",
        "\n",
        "        for combo in combo_indices:\n",
        "            current_mods = {}\n",
        "            combo_str_parts = []\n",
        "            for i, mod in enumerate(valid_mods):\n",
        "                choice = combo[i]\n",
        "                current_mods[mod] = hessian_cache[mod][choice]\n",
        "                combo_str_parts.append(f\"{mod[0]}:{'Inv' if choice else 'Norm'}\")\n",
        "\n",
        "            combo_desc = \"|\".join(combo_str_parts)\n",
        "\n",
        "            fused_H = fuse_hessians_per_scale(current_mods, weights)\n",
        "\n",
        "            coords, neighbors, S = build_frangi_similarity_graph(\n",
        "                fused_H, β, c, c_θ, R,\n",
        "                candidate_mask=None, threshold_mask=threshold_mask, dark_ridges=dark_ridges\n",
        "            )\n",
        "            D = distances_from_similarity(S, mode)\n",
        "\n",
        "            if K == 2:\n",
        "                D = triangle_connectivity_graph(coords, D)\n",
        "\n",
        "            D_cc, idx_nodes = largest_connected_component(D)\n",
        "            sk_pred = np.zeros_like(base, dtype=np.uint8)\n",
        "\n",
        "            if D_cc.shape[0] > 0:\n",
        "                sub_coords = coords[idx_nodes]\n",
        "\n",
        "                Dist = D_cc.copy()\n",
        "                Dist.data = Dist.data ** expZ\n",
        "                Dist = Dist.tocsr()\n",
        "                Dist.setdiag(0.0)\n",
        "\n",
        "                clusterer = hdbscan.HDBSCAN(\n",
        "                    metric=\"precomputed\",\n",
        "                    min_cluster_size=min_cluster_size,\n",
        "                    min_samples=min_samples,\n",
        "                    max_dist=max_dist,\n",
        "                    allow_single_cluster=allow_single_cluster,\n",
        "                )\n",
        "                labels = clusterer.fit_predict(Dist)\n",
        "\n",
        "                all_fault_edges = []\n",
        "                uniq = np.unique(labels)\n",
        "                f_seuil_batch = 0.2\n",
        "                for lab in uniq:\n",
        "                    if lab < 0: continue\n",
        "                    cl = np.where(labels==lab)[0]\n",
        "                    if cl.size < 3: continue\n",
        "\n",
        "                    mst = mst_on_cluster(D_cc, cl)\n",
        "\n",
        "                    # S correspond à tout le graphe.\n",
        "                    # On veut S restreint au cluster 'cl' (qui réfère à D_cc, qui réfère à idx_nodes)\n",
        "                    global_indices = idx_nodes[cl]\n",
        "                    S_cluster = S[global_indices, :][:, global_indices]\n",
        "\n",
        "                    # Betweenness Centrality Filtering\n",
        "                    nodes_kept, skel_graph = extract_backbone_centrality(mst, f_threshold=f_seuil_batch, S=S_cluster, take_similarity=True)\n",
        "                    segs = skeleton_from_mst_graph(skel_graph, sub_coords[cl], nodes_kept, S=S_cluster, take_similarity=True)\n",
        "\n",
        "                    if segs.shape[0] > 0:\n",
        "                        all_fault_edges.append(segs)\n",
        "\n",
        "                if all_fault_edges:\n",
        "                    fault_edges = np.vstack(all_fault_edges)\n",
        "                    mask = np.zeros_like(base, dtype=np.uint8)\n",
        "                    for e in fault_edges[fault_edges[:,-1] <= τ]:\n",
        "                        r0,c0,r1,c1,w = e\n",
        "                        rr = np.linspace(r0, r1, num=int(max(abs(r1-r0),abs(c1-c0))+1)).astype(int)\n",
        "                        cc = np.linspace(c0, c1, num=rr.shape[0]).astype(int)\n",
        "                        rr = np.clip(rr, 0, base.shape[0]-1)\n",
        "                        cc = np.clip(cc, 0, base.shape[1]-1)\n",
        "                        mask[rr,cc] = 1\n",
        "                    sk_pred = skeletonize_lee(mask)\n",
        "\n",
        "            sk_pred_thick = thicken(sk_pred, pixels=épaisseur_trait)\n",
        "            tvs = tversky_index(sk_pred_thick, sk_gt_thick, alpha=1.0, beta=0.5)\n",
        "\n",
        "            if tvs > best_tversky:\n",
        "                best_tversky = tvs\n",
        "                jac = jaccard_index(sk_pred_thick, sk_gt_thick)\n",
        "                wass = wasserstein_distance_skeletons(sk_pred_thick, sk_gt_thick)\n",
        "                best_metrics = {\n",
        "                    \"Image\": img_name,\n",
        "                    \"Jaccard\": jac,\n",
        "                    \"Tversky\": tvs,\n",
        "                    \"Wasserstein\": wass,\n",
        "                    \"Best_Combo\": combo_desc,\n",
        "                    **csd_metrics\n",
        "                }\n",
        "                best_sk_img = Image.fromarray((sk_pred * 255).astype(np.uint8))\n",
        "                best_sk_pred_thick = sk_pred_thick\n",
        "\n",
        "        plt.title(f\"Overlay {img_name}\\nRed: GT, Blue: Ours, Green: CSD\");\n",
        "        plt.imshow(sk_gt_thick*255, cmap='Reds', alpha=0.7)\n",
        "        plt.imshow(best_sk_pred_thick*255, cmap='Blues', alpha=0.5)\n",
        "        if sk_csd_thick is not None:\n",
        "            plt.imshow(sk_csd_thick*255, cmap='Greens', alpha=0.4)\n",
        "        plt.axis('off')\n",
        "        # Save the overlay image\n",
        "        plt.savefig(os.path.join(overlay_dir, f\"{img_name}_overlay.png\"))\n",
        "        # plt.show()\n",
        "        # Clear the current figure to prevent plots from accumulating in memory\n",
        "        plt.clf()\n",
        "        print(f\"Best metrics {img_name} :\", best_metrics)\n",
        "        # Save Best Image immediately\n",
        "        if best_sk_img is not None:\n",
        "            best_sk_img.save(os.path.join(skeletons_dir, f\"{img_name}_skeleton.png\"))\n",
        "        else:\n",
        "            Image.new('L', (base.shape[1], base.shape[0])).save(os.path.join(skeletons_dir, f\"{img_name}_skeleton.png\"))\n",
        "\n",
        "        return best_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return error info but don't crash main loop\n",
        "        return {\n",
        "            \"Image\": f\"im{idx+1:05d}\",\n",
        "            \"Jaccard\": np.nan, \"Tversky\": np.nan, \"Wasserstein\": np.nan, \"Best_Combo\": f\"ERROR: {e}\"\n",
        "        }\n",
        "\n",
        "# --- Batch Processing ---\n",
        "print(f\"Starting parallel batch processing for images {start_idx+1} to {end_idx} using {n_jobs} jobs...\")\n",
        "print(\"Note: Processing 16 combinations (normal/inverted) per image.\")\n",
        "\n",
        "with tqdm_joblib(tqdm(desc=\"Progress\", total=end_idx-start_idx)) as progress_bar:\n",
        "    metrics_data = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_image_idx)(i) for i in range(start_idx, end_idx)\n",
        "    )\n",
        "\n",
        "# --- Save Summary CSV ---\n",
        "df = pd.DataFrame(metrics_data)\n",
        "if not df.empty:\n",
        "    cols_to_mean = [\"Jaccard\", \"Tversky\", \"Wasserstein\", \"CSD_Jaccard\", \"CSD_Tversky\", \"CSD_Wasserstein\"]\n",
        "    # Filter cols that exist\n",
        "    cols_to_mean = [c for c in cols_to_mean if c in df.columns]\n",
        "    mean_vals = df[cols_to_mean].mean()\n",
        "\n",
        "    mean_dict = {\"Image\": \"AVERAGE\", \"Best_Combo\": \"-\"}\n",
        "    for c in cols_to_mean:\n",
        "        mean_dict[c] = mean_vals[c]\n",
        "\n",
        "    mean_row = pd.DataFrame([mean_dict])\n",
        "    df = pd.concat([df, mean_row], ignore_index=True)\n",
        "\n",
        "csv_path = os.path.join(output_dir, \"metrics_summary.csv\")\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Saved metrics summary to {csv_path}\")\n",
        "print(\"Averages:\")\n",
        "print(mean_vals if not df.empty else \"No data\")"
      ],
      "id": "59SBwqImZvWZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fe209d"
      },
      "source": [
        "# Results\n"
      ],
      "id": "97fe209d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78dadc9"
      },
      "source": [
        "import os\n",
        "\n",
        "results_base_dir = '/content/drive/MyDrive/Datasets/FIND/Results'\n",
        "\n",
        "# List all items in the base directory\n",
        "all_items = os.listdir(results_base_dir)\n",
        "\n",
        "# Filter to keep only directories\n",
        "result_dirs = [item for item in all_items if os.path.isdir(os.path.join(results_base_dir, item))]\n",
        "\n",
        "print(f\"Found {len(result_dirs)} subdirectories in '{results_base_dir}':\")\n",
        "for d in result_dirs:\n",
        "    print(f\"- {d}\")"
      ],
      "id": "a78dadc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac3874d2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "metrics_summary = []\n",
        "\n",
        "for d in result_dirs:\n",
        "    folder_path = os.path.join(results_base_dir, d)\n",
        "    csv_path = os.path.join(folder_path, 'metrics_summary.csv')\n",
        "\n",
        "    if os.path.exists(csv_path):\n",
        "        df_metrics = pd.read_csv(csv_path)\n",
        "        average_row = df_metrics[df_metrics['Image'] == 'AVERAGE']\n",
        "\n",
        "        if not average_row.empty:\n",
        "            # Extract relevant metrics and folder name\n",
        "            jaccard = average_row['Jaccard'].values[0]\n",
        "            tversky = average_row['Tversky'].values[0]\n",
        "            wasserstein = average_row['Wasserstein'].values[0]\n",
        "\n",
        "            metrics_summary.append({\n",
        "                'Configuration': d,\n",
        "                'Jaccard': jaccard,\n",
        "                'Tversky': tversky,\n",
        "                'Wasserstein': wasserstein\n",
        "            })\n",
        "    else:\n",
        "        print(f\"Warning: 'metrics_summary.csv' not found in {d}\")\n",
        "\n",
        "# Compile the results into a single DataFrame\n",
        "summary_df = pd.DataFrame(metrics_summary)\n",
        "\n",
        "print(\"Compiled metrics summary:\")\n",
        "print(summary_df)"
      ],
      "id": "ac3874d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b703dde"
      },
      "source": [
        "summary_df_sorted = summary_df.sort_values(by='Tversky', ascending=False)\n",
        "print(\"Sorted metrics summary by Tversky index (descending):\")\n",
        "print(summary_df_sorted)"
      ],
      "id": "0b703dde",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}